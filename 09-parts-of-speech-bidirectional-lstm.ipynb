{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:23.928021Z",
     "start_time": "2017-10-16T04:27:23.837860Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:24.011942Z",
     "start_time": "2017-10-16T04:27:23.930839Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0 or line[0] == '#':\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split('\\t')\n",
    "        sentence.append([splits[1], splits[2]])\n",
    "    \n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:25.593435Z",
     "start_time": "2017-10-16T04:27:24.015818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pos-tagging-lstm/NER-de-train.tsv 24000 sentences\n",
      "./pos-tagging-lstm/NER-de-dev.tsv 2200 sentences\n",
      "./pos-tagging-lstm/NER-de-test.tsv 5100 sentences\n"
     ]
    }
   ],
   "source": [
    "trainSentences = readFile('./pos-tagging-lstm/NER-de-train.tsv')\n",
    "devSentences   = readFile('./pos-tagging-lstm/NER-de-dev.tsv')\n",
    "testSentences  = readFile('./pos-tagging-lstm/NER-de-test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:25.622316Z",
     "start_time": "2017-10-16T04:27:25.596689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Schartau', 'B-PER'],\n",
       " ['sagte', 'O'],\n",
       " ['dem', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['Tagesspiegel', 'B-ORG'],\n",
       " ['\"', 'O'],\n",
       " ['vom', 'O'],\n",
       " ['Freitag', 'O'],\n",
       " [',', 'O'],\n",
       " ['Fischer', 'B-PER'],\n",
       " ['sei', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['in', 'O'],\n",
       " ['einer', 'O'],\n",
       " ['Weise', 'O'],\n",
       " ['aufgetreten', 'O'],\n",
       " [',', 'O'],\n",
       " ['die', 'O'],\n",
       " ['alles', 'O'],\n",
       " ['andere', 'O'],\n",
       " ['als', 'O'],\n",
       " ['Ã¼berzeugend', 'O'],\n",
       " ['war', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:26.091726Z",
     "start_time": "2017-10-16T04:27:25.624800Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_set = set()\n",
    "unique_words = set()\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, label in sentence:\n",
    "            POS_set.add(label)\n",
    "            unique_words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:26.098907Z",
     "start_time": "2017-10-16T04:27:26.093865Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_index_dict = dict([(x,i) for i,x in enumerate(POS_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:26.320154Z",
     "start_time": "2017-10-16T04:27:26.104377Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:26.382208Z",
     "start_time": "2017-10-16T04:27:26.323274Z"
    }
   },
   "outputs": [],
   "source": [
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:27:26.491223Z",
     "start_time": "2017-10-16T04:27:26.385946Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = {}\n",
    "wordEmbeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:28:59.893774Z",
     "start_time": "2017-10-16T04:28:59.888434Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:29:38.558970Z",
     "start_time": "2017-10-16T04:29:18.033964Z"
    }
   },
   "outputs": [],
   "source": [
    "fEmbeddings = gzip.open('./pos-tagging-lstm/2014_tudarmstadt_german_50mincount.vocab.gz', \"r\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word_index_dict) == 0: #Add padding+unknown\n",
    "        word_index_dict[\"PADDING_TOKEN\"] = len(word_index_dict)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word_index_dict[\"UNKNOWN_TOKEN\"] = len(word_index_dict)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in unique_words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word_index_dict[split[0]] = len(word_index_dict)\n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:32:09.059860Z",
     "start_time": "2017-10-16T04:32:09.015331Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:32:09.936211Z",
     "start_time": "2017-10-16T04:32:09.895411Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    unknownIdx = word_index_dict['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word_index_dict['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word, label in sentence:  \n",
    "            if word in word_index_dict:\n",
    "                wordIdx = word_index_dict[word]\n",
    "            elif word.lower() in word_index_dict:\n",
    "                wordIdx = word_index_dict[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "            \n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            labelIndices.append(POS_index_dict[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, labelIndices]) \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:32:12.492443Z",
     "start_time": "2017-10-16T04:32:10.518482Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)\n",
    "dev_set   = createMatrices(devSentences, windowsize=3)\n",
    "test_set  = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:33:38.997602Z",
     "start_time": "2017-10-16T04:33:21.882298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:33:39.121275Z",
     "start_time": "2017-10-16T04:33:38.999998Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0],\n",
    "                  output_dim=wordEmbeddings.shape[1],\n",
    "                  weights=[wordEmbeddings],\n",
    "                  trainable=False)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:34:17.640577Z",
     "start_time": "2017-10-16T04:34:17.616417Z"
    }
   },
   "outputs": [],
   "source": [
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1],\n",
    "                   input_dim=caseEmbeddings.shape[0],\n",
    "                   weights=[caseEmbeddings],\n",
    "                   trainable=False)(casing_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:34:42.689195Z",
     "start_time": "2017-10-16T04:34:41.986982Z"
    }
   },
   "outputs": [],
   "source": [
    "output = concatenate([words, casing])\n",
    "output = Bidirectional(LSTM(50, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(POS_index_dict), activation='softmax'))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:34:59.322755Z",
     "start_time": "2017-10-16T04:34:59.280412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "casing_input (InputLayer)        (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, None, 100)     6670600     words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, None, 8)       64          casing_input[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, None, 108)     0           embedding_1[0][0]                \n",
      "                                                                   embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, None, 100)     63600       concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, None, 25)      2525        bidirectional_2[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 6,736,789\n",
      "Trainable params: 66,125\n",
      "Non-trainable params: 6,670,664\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[words_input, casing_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:36:03.014902Z",
     "start_time": "2017-10-16T04:36:02.964027Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(dataset): \n",
    "    endIdx = len(dataset)\n",
    "    \n",
    "    for idx in range(endIdx):\n",
    "        tokens, casing, labels = dataset[idx]        \n",
    "            \n",
    "        labels = np.expand_dims([labels], -1)     \n",
    "        yield labels, np.asarray([tokens]), np.asarray([casing])\n",
    "\n",
    "\n",
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    for tokens, casing, labels in dataset:    \n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        pred = model.predict([tokens, casing], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        \n",
    "        \n",
    "    return predLabels, correctLabels\n",
    "        \n",
    "number_of_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:36:18.949079Z",
     "start_time": "2017-10-16T04:36:18.945094Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:37:00.950991Z",
     "start_time": "2017-10-16T04:37:00.944311Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:48:35.013984Z",
     "start_time": "2017-10-16T04:48:35.008435Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in POS_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T04:59:32.642806Z",
     "start_time": "2017-10-16T04:59:32.636834Z"
    }
   },
   "outputs": [],
   "source": [
    "import BIOF1Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T06:58:41.011591Z",
     "start_time": "2017-10-16T04:59:33.271474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Epoch 0 -----------\n",
      "571.16 sec for training                 \n",
      "Dev-Data: Prec: 0.746, Rec: 0.701, F1: 0.723\n",
      "Test-Data: Prec: 0.752, Rec: 0.685, F1: 0.717\n",
      "595.81 sec for evaluation\n",
      "\n",
      "--------- Epoch 1 -----------\n",
      "572.81 sec for training                 \n",
      "Dev-Data: Prec: 0.761, Rec: 0.716, F1: 0.738\n",
      "Test-Data: Prec: 0.758, Rec: 0.701, F1: 0.728\n",
      "599.09 sec for evaluation\n",
      "\n",
      "--------- Epoch 2 -----------\n",
      "624.98 sec for training                 \n",
      "Dev-Data: Prec: 0.737, Rec: 0.712, F1: 0.724\n",
      "Test-Data: Prec: 0.742, Rec: 0.704, F1: 0.722\n",
      "651.89 sec for evaluation\n",
      "\n",
      "--------- Epoch 3 -----------\n",
      "653.36 sec for training                 \n",
      "Dev-Data: Prec: 0.755, Rec: 0.720, F1: 0.737\n",
      "Test-Data: Prec: 0.750, Rec: 0.702, F1: 0.725\n",
      "683.67 sec for evaluation\n",
      "\n",
      "--------- Epoch 4 -----------\n",
      "610.46 sec for training                 \n",
      "Dev-Data: Prec: 0.744, Rec: 0.731, F1: 0.738\n",
      "Test-Data: Prec: 0.737, Rec: 0.714, F1: 0.725\n",
      "635.35 sec for evaluation\n",
      "\n",
      "--------- Epoch 5 -----------\n",
      "595.66 sec for training                 \n",
      "Dev-Data: Prec: 0.793, Rec: 0.715, F1: 0.752\n",
      "Test-Data: Prec: 0.787, Rec: 0.696, F1: 0.739\n",
      "620.16 sec for evaluation\n",
      "\n",
      "--------- Epoch 6 -----------\n",
      "586.01 sec for training                 \n",
      "Dev-Data: Prec: 0.767, Rec: 0.716, F1: 0.741\n",
      "Test-Data: Prec: 0.763, Rec: 0.702, F1: 0.731\n",
      "611.39 sec for evaluation\n",
      "\n",
      "--------- Epoch 7 -----------\n",
      "632.89 sec for training                 \n",
      "Dev-Data: Prec: 0.769, Rec: 0.716, F1: 0.742\n",
      "Test-Data: Prec: 0.770, Rec: 0.704, F1: 0.735\n",
      "663.06 sec for evaluation\n",
      "\n",
      "--------- Epoch 8 -----------\n",
      "658.96 sec for training                 \n",
      "Dev-Data: Prec: 0.771, Rec: 0.727, F1: 0.748\n",
      "Test-Data: Prec: 0.768, Rec: 0.709, F1: 0.737\n",
      "684.63 sec for evaluation\n",
      "\n",
      "--------- Epoch 9 -----------\n",
      "638.94 sec for training                 \n",
      "Dev-Data: Prec: 0.776, Rec: 0.729, F1: 0.752\n",
      "Test-Data: Prec: 0.769, Rec: 0.707, F1: 0.737\n",
      "663.58 sec for evaluation\n",
      "\n",
      "--------- Epoch 10 -----------\n",
      "607.51 sec for training                 \n",
      "Dev-Data: Prec: 0.758, Rec: 0.733, F1: 0.745\n",
      "Test-Data: Prec: 0.751, Rec: 0.712, F1: 0.731\n",
      "631.87 sec for evaluation\n",
      "\n",
      "--------- Epoch 11 -----------\n",
      "Sentence: 4000 / 24000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0b1cbdbf02ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(number_of_epochs):    \n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_set)\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    #Train one sentence at a time (i.e. online training) to avoid padding of sentences\n",
    "    cnt = 0\n",
    "    for batch in iterate_minibatches(train_set):\n",
    "        labels, tokens, casing = batch       \n",
    "        model.train_on_batch([tokens, casing], labels)   \n",
    "        cnt += 1\n",
    "        \n",
    "        if cnt % 100 == 0:\n",
    "            print('Sentence: %d / %d' % (cnt, len(train_set)), end='\\r')\n",
    "    print(\"%.2f sec for training                 \" % (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    #Performance on dev dataset        \n",
    "    predLabels, correctLabels = tag_dataset(dev_set)        \n",
    "    pre_dev, rec_dev, f1_dev = BIOF1Validation.compute_f1(predLabels, correctLabels, idx2Label)\n",
    "    print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n",
    "    \n",
    "    #Performance on test dataset       \n",
    "    predLabels, correctLabels = tag_dataset(test_set)        \n",
    "    pre_test, rec_test, f1_test= BIOF1Validation.compute_f1(predLabels, correctLabels, idx2Label)\n",
    "    print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))\n",
    "    \n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))\n",
    "    print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
