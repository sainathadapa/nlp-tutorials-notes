{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/UKPLab/deeplearning4nlp-tutorial/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:08.529780Z",
     "start_time": "2018-04-25T16:44:08.176967Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:08.983692Z",
     "start_time": "2018-04-25T16:44:08.971029Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0 or line[0] == '#':\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split('\\t')\n",
    "        sentence.append([splits[1], splits[2]])\n",
    "    \n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:10.742644Z",
     "start_time": "2018-04-25T16:44:09.429593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pos-tagging-lstm/NER-de-train.tsv 24000 sentences\n",
      "data/pos-tagging-lstm/NER-de-dev.tsv 2200 sentences\n",
      "data/pos-tagging-lstm/NER-de-test.tsv 5100 sentences\n"
     ]
    }
   ],
   "source": [
    "trainSentences = readFile('data/pos-tagging-lstm/NER-de-train.tsv')\n",
    "devSentences   = readFile('data/pos-tagging-lstm/NER-de-dev.tsv')\n",
    "testSentences  = readFile('data/pos-tagging-lstm/NER-de-test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:10.760188Z",
     "start_time": "2018-04-25T16:44:10.744599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Schartau', 'B-PER'],\n",
       " ['sagte', 'O'],\n",
       " ['dem', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['Tagesspiegel', 'B-ORG'],\n",
       " ['\"', 'O'],\n",
       " ['vom', 'O'],\n",
       " ['Freitag', 'O'],\n",
       " [',', 'O'],\n",
       " ['Fischer', 'B-PER'],\n",
       " ['sei', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['in', 'O'],\n",
       " ['einer', 'O'],\n",
       " ['Weise', 'O'],\n",
       " ['aufgetreten', 'O'],\n",
       " [',', 'O'],\n",
       " ['die', 'O'],\n",
       " ['alles', 'O'],\n",
       " ['andere', 'O'],\n",
       " ['als', 'O'],\n",
       " ['Ã¼berzeugend', 'O'],\n",
       " ['war', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:11.714960Z",
     "start_time": "2018-04-25T16:44:11.375086Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_set = set()\n",
    "unique_words = set()\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, label in sentence:\n",
    "            POS_set.add(label)\n",
    "            unique_words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:12.157215Z",
     "start_time": "2018-04-25T16:44:12.151093Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_index_dict = dict([(x,i) for i,x in enumerate(POS_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:12.622734Z",
     "start_time": "2018-04-25T16:44:12.563732Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:12.928030Z",
     "start_time": "2018-04-25T16:44:12.921483Z"
    }
   },
   "outputs": [],
   "source": [
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:13.686123Z",
     "start_time": "2018-04-25T16:44:13.671077Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = {}\n",
    "wordEmbeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:14.026892Z",
     "start_time": "2018-04-25T16:44:14.019786Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:29.991289Z",
     "start_time": "2018-04-25T16:44:14.695396Z"
    }
   },
   "outputs": [],
   "source": [
    "fEmbeddings = gzip.open('data/pos-tagging-lstm/2014_tudarmstadt_german_50mincount.vocab.gz', \"r\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word_index_dict) == 0: #Add padding+unknown\n",
    "        word_index_dict[\"PADDING_TOKEN\"] = len(word_index_dict)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word_index_dict[\"UNKNOWN_TOKEN\"] = len(word_index_dict)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in unique_words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word_index_dict[split[0]] = len(word_index_dict)\n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:30.006161Z",
     "start_time": "2018-04-25T16:44:29.995710Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:30.156201Z",
     "start_time": "2018-04-25T16:44:30.008621Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    unknownIdx = word_index_dict['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word_index_dict['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word, label in sentence:  \n",
    "            if word in word_index_dict:\n",
    "                wordIdx = word_index_dict[word]\n",
    "            elif word.lower() in word_index_dict:\n",
    "                wordIdx = word_index_dict[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "            \n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            labelIndices.append(POS_index_dict[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, labelIndices]) \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:32.090129Z",
     "start_time": "2018-04-25T16:44:30.160971Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)\n",
    "dev_set   = createMatrices(devSentences, windowsize=3)\n",
    "test_set  = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:34.120655Z",
     "start_time": "2018-04-25T16:44:32.091982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sainath/code/nlp-tutorials-notes/.env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:45.669981Z",
     "start_time": "2018-04-25T16:44:44.489246Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0],\n",
    "                  output_dim=wordEmbeddings.shape[1],\n",
    "                  weights=[wordEmbeddings],\n",
    "                  trainable=False)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:46.507494Z",
     "start_time": "2018-04-25T16:44:46.470132Z"
    }
   },
   "outputs": [],
   "source": [
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1],\n",
    "                   input_dim=caseEmbeddings.shape[0],\n",
    "                   weights=[caseEmbeddings],\n",
    "                   trainable=False)(casing_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:47.562772Z",
     "start_time": "2018-04-25T16:44:47.002268Z"
    }
   },
   "outputs": [],
   "source": [
    "output = concatenate([words, casing])\n",
    "output = Bidirectional(LSTM(50, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(POS_index_dict), activation='softmax'))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:47.603969Z",
     "start_time": "2018-04-25T16:44:47.564655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    6670600     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 108)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 100)    63600       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 25)     2525        bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 6,736,789\n",
      "Trainable params: 66,125\n",
      "Non-trainable params: 6,670,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[words_input, casing_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:48.323147Z",
     "start_time": "2018-04-25T16:44:48.310368Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(dataset): \n",
    "    endIdx = len(dataset)\n",
    "    \n",
    "    for idx in range(endIdx):\n",
    "        tokens, casing, labels = dataset[idx]        \n",
    "            \n",
    "        labels = np.expand_dims([labels], -1)     \n",
    "        yield labels, np.asarray([tokens]), np.asarray([casing])\n",
    "\n",
    "\n",
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    for tokens, casing, labels in dataset:    \n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        pred = model.predict([tokens, casing], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        \n",
    "        \n",
    "    return predLabels, correctLabels\n",
    "        \n",
    "number_of_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:48.865220Z",
     "start_time": "2018-04-25T16:44:48.859845Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:49.368312Z",
     "start_time": "2018-04-25T16:44:49.363213Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:49.765907Z",
     "start_time": "2018-04-25T16:44:49.759727Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in POS_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:44:50.218977Z",
     "start_time": "2018-04-25T16:44:50.137636Z"
    }
   },
   "outputs": [],
   "source": [
    "import BIOF1Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T04:38:52.648729Z",
     "start_time": "2018-04-25T16:44:51.491823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Epoch 0 -----------\n",
      "2090.18 sec for training                 \n",
      "Dev-Data: Prec: 0.715, Rec: 0.656, F1: 0.684\n",
      "Test-Data: Prec: 0.722, Rec: 0.642, F1: 0.680\n",
      "2243.84 sec for evaluation\n",
      "\n",
      "--------- Epoch 1 -----------\n",
      "2012.05 sec for training                 \n",
      "Dev-Data: Prec: 0.761, Rec: 0.695, F1: 0.727\n",
      "Test-Data: Prec: 0.753, Rec: 0.682, F1: 0.716\n",
      "2167.01 sec for evaluation\n",
      "\n",
      "--------- Epoch 2 -----------\n",
      "1997.61 sec for training                 \n",
      "Dev-Data: Prec: 0.775, Rec: 0.695, F1: 0.733\n",
      "Test-Data: Prec: 0.771, Rec: 0.674, F1: 0.719\n",
      "2151.74 sec for evaluation\n",
      "\n",
      "--------- Epoch 3 -----------\n",
      "1991.33 sec for training                 \n",
      "Dev-Data: Prec: 0.754, Rec: 0.690, F1: 0.721\n",
      "Test-Data: Prec: 0.756, Rec: 0.679, F1: 0.716\n",
      "2146.35 sec for evaluation\n",
      "\n",
      "--------- Epoch 4 -----------\n",
      "1984.95 sec for training                 \n",
      "Dev-Data: Prec: 0.753, Rec: 0.721, F1: 0.737\n",
      "Test-Data: Prec: 0.753, Rec: 0.707, F1: 0.730\n",
      "2139.45 sec for evaluation\n",
      "\n",
      "--------- Epoch 5 -----------\n",
      "1980.34 sec for training                 \n",
      "Dev-Data: Prec: 0.762, Rec: 0.715, F1: 0.738\n",
      "Test-Data: Prec: 0.753, Rec: 0.697, F1: 0.724\n",
      "2134.76 sec for evaluation\n",
      "\n",
      "--------- Epoch 6 -----------\n",
      "1978.18 sec for training                 \n",
      "Dev-Data: Prec: 0.781, Rec: 0.720, F1: 0.749\n",
      "Test-Data: Prec: 0.763, Rec: 0.688, F1: 0.723\n",
      "2133.33 sec for evaluation\n",
      "\n",
      "--------- Epoch 7 -----------\n",
      "1976.10 sec for training                 \n",
      "Dev-Data: Prec: 0.787, Rec: 0.729, F1: 0.757\n",
      "Test-Data: Prec: 0.767, Rec: 0.699, F1: 0.732\n",
      "2131.00 sec for evaluation\n",
      "\n",
      "--------- Epoch 8 -----------\n",
      "1975.74 sec for training                 \n",
      "Dev-Data: Prec: 0.771, Rec: 0.717, F1: 0.743\n",
      "Test-Data: Prec: 0.766, Rec: 0.694, F1: 0.728\n",
      "2130.43 sec for evaluation\n",
      "\n",
      "--------- Epoch 9 -----------\n",
      "1975.35 sec for training                 \n",
      "Dev-Data: Prec: 0.772, Rec: 0.728, F1: 0.749\n",
      "Test-Data: Prec: 0.762, Rec: 0.701, F1: 0.730\n",
      "2130.08 sec for evaluation\n",
      "\n",
      "--------- Epoch 10 -----------\n",
      "1972.77 sec for training                 \n",
      "Dev-Data: Prec: 0.767, Rec: 0.720, F1: 0.743\n",
      "Test-Data: Prec: 0.760, Rec: 0.702, F1: 0.730\n",
      "2127.60 sec for evaluation\n",
      "\n",
      "--------- Epoch 11 -----------\n",
      "1973.17 sec for training                 \n",
      "Dev-Data: Prec: 0.778, Rec: 0.717, F1: 0.746\n",
      "Test-Data: Prec: 0.775, Rec: 0.695, F1: 0.733\n",
      "2128.18 sec for evaluation\n",
      "\n",
      "--------- Epoch 12 -----------\n",
      "1980.34 sec for training                 \n",
      "Dev-Data: Prec: 0.791, Rec: 0.715, F1: 0.751\n",
      "Test-Data: Prec: 0.774, Rec: 0.691, F1: 0.730\n",
      "2136.64 sec for evaluation\n",
      "\n",
      "--------- Epoch 13 -----------\n",
      "1981.57 sec for training                 \n",
      "Dev-Data: Prec: 0.772, Rec: 0.712, F1: 0.741\n",
      "Test-Data: Prec: 0.771, Rec: 0.699, F1: 0.734\n",
      "2137.92 sec for evaluation\n",
      "\n",
      "--------- Epoch 14 -----------\n",
      "1978.45 sec for training                 \n",
      "Dev-Data: Prec: 0.789, Rec: 0.720, F1: 0.753\n",
      "Test-Data: Prec: 0.773, Rec: 0.696, F1: 0.733\n",
      "2135.47 sec for evaluation\n",
      "\n",
      "--------- Epoch 15 -----------\n",
      "1979.42 sec for training                 \n",
      "Dev-Data: Prec: 0.754, Rec: 0.721, F1: 0.737\n",
      "Test-Data: Prec: 0.746, Rec: 0.699, F1: 0.722\n",
      "2135.52 sec for evaluation\n",
      "\n",
      "--------- Epoch 16 -----------\n",
      "1978.30 sec for training                 \n",
      "Dev-Data: Prec: 0.776, Rec: 0.716, F1: 0.745\n",
      "Test-Data: Prec: 0.776, Rec: 0.695, F1: 0.733\n",
      "2134.86 sec for evaluation\n",
      "\n",
      "--------- Epoch 17 -----------\n",
      "1975.46 sec for training                 \n",
      "Dev-Data: Prec: 0.750, Rec: 0.730, F1: 0.740\n",
      "Test-Data: Prec: 0.748, Rec: 0.702, F1: 0.724\n",
      "2131.72 sec for evaluation\n",
      "\n",
      "--------- Epoch 18 -----------\n",
      "1975.80 sec for training                 \n",
      "Dev-Data: Prec: 0.774, Rec: 0.736, F1: 0.755\n",
      "Test-Data: Prec: 0.750, Rec: 0.703, F1: 0.726\n",
      "2132.33 sec for evaluation\n",
      "\n",
      "--------- Epoch 19 -----------\n",
      "1976.28 sec for training                 \n",
      "Dev-Data: Prec: 0.760, Rec: 0.726, F1: 0.743\n",
      "Test-Data: Prec: 0.751, Rec: 0.701, F1: 0.725\n",
      "2132.29 sec for evaluation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(number_of_epochs):    \n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_set)\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    #Train one sentence at a time (i.e. online training) to avoid padding of sentences\n",
    "    cnt = 0\n",
    "    for batch in iterate_minibatches(train_set):\n",
    "        labels, tokens, casing = batch       \n",
    "        model.train_on_batch([tokens, casing], labels)   \n",
    "        cnt += 1\n",
    "        \n",
    "        if cnt % 100 == 0:\n",
    "            print('Sentence: %d / %d' % (cnt, len(train_set)), end='\\r')\n",
    "    print(\"%.2f sec for training                 \" % (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    #Performance on dev dataset        \n",
    "    predLabels, correctLabels = tag_dataset(dev_set)        \n",
    "    pre_dev, rec_dev, f1_dev = BIOF1Validation.compute_f1(predLabels, correctLabels, idx2Label)\n",
    "    print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n",
    "    \n",
    "    #Performance on test dataset       \n",
    "    predLabels, correctLabels = tag_dataset(test_set)        \n",
    "    pre_test, rec_test, f1_test= BIOF1Validation.compute_f1(predLabels, correctLabels, idx2Label)\n",
    "    print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))\n",
    "    \n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))\n",
    "    print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
