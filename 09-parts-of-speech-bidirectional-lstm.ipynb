{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:16.063919Z",
     "start_time": "2017-11-06T07:50:15.993122Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:16.353835Z",
     "start_time": "2017-11-06T07:50:16.316535Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0 or line[0] == '#':\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split('\\t')\n",
    "        sentence.append([splits[1], splits[2]])\n",
    "    \n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:26.354974Z",
     "start_time": "2017-11-06T07:50:24.873186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pos-tagging-lstm/NER-de-train.tsv 24000 sentences\n",
      "data/pos-tagging-lstm/NER-de-dev.tsv 2200 sentences\n",
      "data/pos-tagging-lstm/NER-de-test.tsv 5100 sentences\n"
     ]
    }
   ],
   "source": [
    "trainSentences = readFile('data/pos-tagging-lstm/NER-de-train.tsv')\n",
    "devSentences   = readFile('data/pos-tagging-lstm/NER-de-dev.tsv')\n",
    "testSentences  = readFile('data/pos-tagging-lstm/NER-de-test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:27.394677Z",
     "start_time": "2017-11-06T07:50:27.373279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Schartau', 'B-PER'],\n",
       " ['sagte', 'O'],\n",
       " ['dem', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['Tagesspiegel', 'B-ORG'],\n",
       " ['\"', 'O'],\n",
       " ['vom', 'O'],\n",
       " ['Freitag', 'O'],\n",
       " [',', 'O'],\n",
       " ['Fischer', 'B-PER'],\n",
       " ['sei', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['in', 'O'],\n",
       " ['einer', 'O'],\n",
       " ['Weise', 'O'],\n",
       " ['aufgetreten', 'O'],\n",
       " [',', 'O'],\n",
       " ['die', 'O'],\n",
       " ['alles', 'O'],\n",
       " ['andere', 'O'],\n",
       " ['als', 'O'],\n",
       " ['Ã¼berzeugend', 'O'],\n",
       " ['war', 'O'],\n",
       " ['\"', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:28.906449Z",
     "start_time": "2017-11-06T07:50:28.560752Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_set = set()\n",
    "unique_words = set()\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, label in sentence:\n",
    "            POS_set.add(label)\n",
    "            unique_words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:29.963444Z",
     "start_time": "2017-11-06T07:50:29.957004Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_index_dict = dict([(x,i) for i,x in enumerate(POS_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:30.714675Z",
     "start_time": "2017-11-06T07:50:30.653299Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:31.204387Z",
     "start_time": "2017-11-06T07:50:31.194445Z"
    }
   },
   "outputs": [],
   "source": [
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:31.910605Z",
     "start_time": "2017-11-06T07:50:31.893870Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = {}\n",
    "wordEmbeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:50:33.009861Z",
     "start_time": "2017-11-06T07:50:33.004230Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:51:03.166656Z",
     "start_time": "2017-11-06T07:50:42.752468Z"
    }
   },
   "outputs": [],
   "source": [
    "fEmbeddings = gzip.open('data/pos-tagging-lstm/2014_tudarmstadt_german_50mincount.vocab.gz', \"r\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word_index_dict) == 0: #Add padding+unknown\n",
    "        word_index_dict[\"PADDING_TOKEN\"] = len(word_index_dict)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word_index_dict[\"UNKNOWN_TOKEN\"] = len(word_index_dict)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in unique_words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word_index_dict[split[0]] = len(word_index_dict)\n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:51:03.205647Z",
     "start_time": "2017-11-06T07:51:03.168430Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:51:03.416368Z",
     "start_time": "2017-11-06T07:51:03.209556Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    unknownIdx = word_index_dict['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word_index_dict['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word, label in sentence:  \n",
    "            if word in word_index_dict:\n",
    "                wordIdx = word_index_dict[word]\n",
    "            elif word.lower() in word_index_dict:\n",
    "                wordIdx = word_index_dict[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "            \n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            labelIndices.append(POS_index_dict[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, labelIndices]) \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:51:05.461962Z",
     "start_time": "2017-11-06T07:51:03.420715Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)\n",
    "dev_set   = createMatrices(devSentences, windowsize=3)\n",
    "test_set  = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:28.126818Z",
     "start_time": "2017-11-06T07:52:11.345552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:28.975307Z",
     "start_time": "2017-11-06T07:52:28.128875Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0],\n",
    "                  output_dim=wordEmbeddings.shape[1],\n",
    "                  weights=[wordEmbeddings],\n",
    "                  trainable=False)(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:29.004301Z",
     "start_time": "2017-11-06T07:52:28.980254Z"
    }
   },
   "outputs": [],
   "source": [
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1],\n",
    "                   input_dim=caseEmbeddings.shape[0],\n",
    "                   weights=[caseEmbeddings],\n",
    "                   trainable=False)(casing_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:29.797133Z",
     "start_time": "2017-11-06T07:52:29.006319Z"
    }
   },
   "outputs": [],
   "source": [
    "output = concatenate([words, casing])\n",
    "output = Bidirectional(LSTM(50, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(POS_index_dict), activation='softmax'))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:29.837490Z",
     "start_time": "2017-11-06T07:52:29.798917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "casing_input (InputLayer)        (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, None, 100)     6670600     words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, None, 8)       64          casing_input[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, None, 108)     0           embedding_1[0][0]                \n",
      "                                                                   embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, None, 100)     63600       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, None, 25)      2525        bidirectional_1[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 6,736,789\n",
      "Trainable params: 66,125\n",
      "Non-trainable params: 6,670,664\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[words_input, casing_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:29.988081Z",
     "start_time": "2017-11-06T07:52:29.839268Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(dataset): \n",
    "    endIdx = len(dataset)\n",
    "    \n",
    "    for idx in range(endIdx):\n",
    "        tokens, casing, labels = dataset[idx]        \n",
    "            \n",
    "        labels = np.expand_dims([labels], -1)     \n",
    "        yield labels, np.asarray([tokens]), np.asarray([casing])\n",
    "\n",
    "\n",
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    for tokens, casing, labels in dataset:    \n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        pred = model.predict([tokens, casing], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        \n",
    "        \n",
    "    return predLabels, correctLabels\n",
    "        \n",
    "number_of_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:30.083599Z",
     "start_time": "2017-11-06T07:52:29.990065Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:30.191826Z",
     "start_time": "2017-11-06T07:52:30.087797Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T07:52:30.300337Z",
     "start_time": "2017-11-06T07:52:30.196376Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in POS_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-06T07:52:19.255Z"
    }
   },
   "outputs": [],
   "source": [
    "import BIOF1Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-06T07:52:20.035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Epoch 0 -----------\n",
      "Sentence: 16300 / 24000\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(number_of_epochs):    \n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_set)\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    #Train one sentence at a time (i.e. online training) to avoid padding of sentences\n",
    "    cnt = 0\n",
    "    for batch in iterate_minibatches(train_set):\n",
    "        labels, tokens, casing = batch       \n",
    "        model.train_on_batch([tokens, casing], labels)   \n",
    "        cnt += 1\n",
    "        \n",
    "        if cnt % 100 == 0:\n",
    "            print('Sentence: %d / %d' % (cnt, len(train_set)), end='\\r')\n",
    "    print(\"%.2f sec for training                 \" % (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    #Performance on dev dataset        \n",
    "    predLabels, correctLabels = tag_dataset(dev_set)        \n",
    "    pre_dev, rec_dev, f1_dev = BIOF1Validation.compute_f1(predLabels, correctLabels, idx2Label)\n",
    "    print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n",
    "    \n",
    "    #Performance on test dataset       \n",
    "    predLabels, correctLabels = tag_dataset(test_set)        \n",
    "    pre_test, rec_test, f1_test= BIOF1Validation.compute_f1(predLabels, correctLabels, idx2Label)\n",
    "    print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))\n",
    "    \n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))\n",
    "    print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
