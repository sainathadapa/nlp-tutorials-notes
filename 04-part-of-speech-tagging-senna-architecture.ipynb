{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Read-input-data\" data-toc-modified-id=\"Read-input-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Read input data</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Find-set-of-POSs-and-unique-words\" data-toc-modified-id=\"Find-set-of-POSs-and-unique-words-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Find set of POSs and unique words</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#glove\" data-toc-modified-id=\"glove-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>glove</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Create-context-index-matrices\" data-toc-modified-id=\"Create-context-index-matrices-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Create context index matrices</a></span></li></ul></li><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Create-the-model\" data-toc-modified-id=\"Create-the-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Create the model</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Train\" data-toc-modified-id=\"Train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Add-case-information\" data-toc-modified-id=\"Add-case-information-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Add case information</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:37:39.240254Z",
     "start_time": "2017-11-06T06:37:39.168775Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:37:41.489863Z",
     "start_time": "2017-11-06T06:37:41.457727Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0 or line[0] == '#':\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split('\\t')\n",
    "        sentence.append([splits[0], splits[1]])\n",
    "    \n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:01.771435Z",
     "start_time": "2017-11-06T06:37:58.982735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pos-tagging-senna/train.txt 50545 sentences\n",
      "data/pos-tagging-senna/dev.txt 2506 sentences\n",
      "data/pos-tagging-senna/test.txt 4134 sentences\n"
     ]
    }
   ],
   "source": [
    "trainSentences = readFile('data/pos-tagging-senna/train.txt')\n",
    "devSentences = readFile('data/pos-tagging-senna/dev.txt')\n",
    "testSentences = readFile('data/pos-tagging-senna/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:01.791979Z",
     "start_time": "2017-11-06T06:38:01.773619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Clearly', 'RB'],\n",
       " [',', 'pct'],\n",
       " ['this', 'DT'],\n",
       " ['was', 'BEDZ'],\n",
       " ['a', 'AT'],\n",
       " ['family', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['crisis', 'NN'],\n",
       " ['.', 'pct']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find set of POSs and unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:05.567318Z",
     "start_time": "2017-11-06T06:38:04.934057Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_set = set()\n",
    "unique_words = set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, label in sentence:\n",
    "            POS_set.add(label)\n",
    "            unique_words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:06.570929Z",
     "start_time": "2017-11-06T06:38:06.567191Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_index_dict = dict([(x,i) for i,x in enumerate(POS_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:07.634105Z",
     "start_time": "2017-11-06T06:38:07.629552Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words.add('__PADDING__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:08.440095Z",
     "start_time": "2017-11-06T06:38:08.402060Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:30.102619Z",
     "start_time": "2017-11-06T06:38:15.837279Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_index = {}\n",
    "f = open('data/glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:30.198770Z",
     "start_time": "2017-11-06T06:38:30.105039Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(unique_words), embedding_dim))\n",
    "for word, i in word_index_dict.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:30.207531Z",
     "start_time": "2017-11-06T06:38:30.201325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.093692  ,  0.16071001, -0.2191    , ..., -0.26234999,\n",
       "        -0.40037   ,  0.45387   ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.14023   , -0.040156  , -0.40678999, ...,  0.27313   ,\n",
       "        -0.53763998, -0.0067463 ],\n",
       "       ..., \n",
       "       [ 0.22397   ,  0.064025  ,  0.99896997, ..., -1.23020005,\n",
       "         0.0095587 ,  1.03320003],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:30.376498Z",
     "start_time": "2017-11-06T06:38:30.210026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49816, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create context index matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:30.493835Z",
     "start_time": "2017-11-06T06:38:30.379952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Clearly', 'RB'],\n",
       " [',', 'pct'],\n",
       " ['this', 'DT'],\n",
       " ['was', 'BEDZ'],\n",
       " ['a', 'AT'],\n",
       " ['family', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['crisis', 'NN'],\n",
       " ['.', 'pct']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:30.597436Z",
     "start_time": "2017-11-06T06:38:30.496410Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    xMatrix = []\n",
    "    yVector = []\n",
    "    \n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            wordIndices = []\n",
    "            for pos in range(i-windowsize, i+windowsize+1):\n",
    "                if pos < 0 or pos >= len(sentence):\n",
    "                    wordIndices.append(padding_index)\n",
    "                else:\n",
    "                    word = sentence[pos][0]\n",
    "                    if word.lower() in word_index_dict:\n",
    "                        wordIndices.append(word_index_dict[word.lower()])\n",
    "                    else:\n",
    "                        wordIndices.append(padding_index)\n",
    "            \n",
    "            yVector.append(POS_index_dict[sentence[i][1]])\n",
    "            xMatrix.append(wordIndices)\n",
    "    \n",
    "    return (np.asarray(xMatrix), np.asarray(yVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:39.702581Z",
     "start_time": "2017-11-06T06:38:30.599457Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:39.724670Z",
     "start_time": "2017-11-06T06:38:39.719355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026265, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:39.818797Z",
     "start_time": "2017-11-06T06:38:39.727326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026265,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:41.089444Z",
     "start_time": "2017-11-06T06:38:39.821999Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_set  = createMatrices(devSentences, windowsize=3)\n",
    "test_set = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:57.452441Z",
     "start_time": "2017-11-06T06:38:41.095160Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:58.311205Z",
     "start_time": "2017-11-06T06:38:57.455228Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape = (7,), dtype = 'int32', name = 'words_input')\n",
    "words = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                  weights = [embedding_matrix], trainable = False)(words_input)\n",
    "words = Flatten()(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:38:58.336260Z",
     "start_time": "2017-11-06T06:38:58.313519Z"
    }
   },
   "outputs": [],
   "source": [
    "output = Dense(units = 100, activation = 'tanh')(words)\n",
    "output = Dense(units = len(POS_set), activation = 'softmax')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:39:26.022267Z",
     "start_time": "2017-11-06T06:39:26.015442Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs = words_input, outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:39:27.560920Z",
     "start_time": "2017-11-06T06:39:27.521013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "words_input (InputLayer)     (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 7, 100)            4981600   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               70100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                8484      \n",
      "=================================================================\n",
      "Total params: 5,060,184\n",
      "Trainable params: 78,584\n",
      "Non-trainable params: 4,981,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:28:18.153266Z",
     "start_time": "2017-10-15T15:28:18.150069Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:46:37.236901Z",
     "start_time": "2017-11-06T06:39:39.540014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.3195\n",
      "Dev-Accuracy: 93.17\n",
      "Test-Accuracy: 93.39\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.2039\n",
      "Dev-Accuracy: 93.85\n",
      "Test-Accuracy: 93.77\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.1840\n",
      "Dev-Accuracy: 94.20\n",
      "Test-Accuracy: 94.12\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.1725\n",
      "Dev-Accuracy: 94.33\n",
      "Test-Accuracy: 94.21\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.1646\n",
      "Dev-Accuracy: 94.21\n",
      "Test-Accuracy: 94.19\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.1588\n",
      "Dev-Accuracy: 94.36\n",
      "Test-Accuracy: 94.41\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.1543\n",
      "Dev-Accuracy: 94.33\n",
      "Test-Accuracy: 94.28\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      "36s - loss: 0.1498\n",
      "Dev-Accuracy: 94.33\n",
      "Test-Accuracy: 94.36\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.1468\n",
      "Dev-Accuracy: 94.47\n",
      "Test-Accuracy: 94.42\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      "37s - loss: 0.1436\n",
      "Dev-Accuracy: 94.48\n",
      "Test-Accuracy: 94.36\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "minibatch_size = 128\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "\n",
    "def predict_classes(prediction):\n",
    " return prediction.argmax(axis=-1)\n",
    " \n",
    "for epoch in range(number_of_epochs):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit(train_set[0], train_set[1], epochs=1, batch_size=minibatch_size, verbose=2, shuffle=True)   \n",
    "    \n",
    "    #Predict labels for development set\n",
    "    dev_pred = predict_classes(model.predict(dev_set[0]))\n",
    "    dev_acc = np.sum(dev_pred == dev_set[1]) / float(len(dev_set[1]))\n",
    "    print(\"Dev-Accuracy: %.2f\" % (dev_acc*100))\n",
    "    \n",
    "    #Predict labels for test set\n",
    "    test_pred = predict_classes(model.predict(test_set[0]))\n",
    "    test_acc = np.sum(test_pred == test_set[1]) / float(len(test_set[1]))\n",
    "    print(\"Test-Accuracy: %.2f\" % (test_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add case information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:46:37.248135Z",
     "start_time": "2017-11-06T06:46:37.238669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_index_dict = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4,\n",
    "                   'mainly_numeric':5, 'contains_digit': 6, '__PADDING__':7}\n",
    "caseEmbeddings = np.identity(len(case_index_dict), dtype='float32')\n",
    "caseEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:46:37.369941Z",
     "start_time": "2017-11-06T06:46:37.250172Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCase(word):\n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "    \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "    return case_index_dict[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:46:37.506144Z",
     "start_time": "2017-11-06T06:46:37.374610Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    xMatrix    = []\n",
    "    caseMatrix = []\n",
    "    yVector    = []\n",
    "    \n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            wordIndices = []\n",
    "            caseIndices = []\n",
    "            for pos in range(i-windowsize, i+windowsize+1):\n",
    "                if pos < 0 or pos >= len(sentence):\n",
    "                    wordIndices.append(padding_index)\n",
    "                    caseIndices.append(case_index_dict['__PADDING__'])\n",
    "                else:\n",
    "                    word = sentence[pos][0]\n",
    "                    if word.lower() in word_index_dict:\n",
    "                        wordIndices.append(word_index_dict[word.lower()])\n",
    "                    else:\n",
    "                        wordIndices.append(padding_index)\n",
    "                    caseIndices.append(getCase(word))\n",
    "            \n",
    "            yVector.append(POS_index_dict[sentence[i][1]])\n",
    "            xMatrix.append(wordIndices)\n",
    "            caseMatrix.append(caseIndices)\n",
    "    \n",
    "    return (np.asarray(xMatrix), np.asarray(caseMatrix), np.asarray(yVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:47:05.823917Z",
     "start_time": "2017-11-06T06:46:37.508956Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)\n",
    "dev_set  = createMatrices(devSentences, windowsize=3)\n",
    "test_set = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:47:05.834053Z",
     "start_time": "2017-11-06T06:47:05.827817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1026265, 7)\n",
      "(1026265, 7)\n",
      "(1026265,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0].shape)\n",
    "print(train_set[1].shape)\n",
    "print(train_set[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:47:06.058608Z",
     "start_time": "2017-11-06T06:47:05.836380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, 7)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "cases_input (InputLayer)         (None, 7)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 7, 100)        4981600     words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 7, 8)          64          cases_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 700)           0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 56)            0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 756)           0           flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 100)           75700       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 84)            8484        dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 5,065,848\n",
      "Trainable params: 84,184\n",
      "Non-trainable params: 4,981,664\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape = (7,), dtype = 'int32', name = 'words_input')\n",
    "words = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                  weights = [embedding_matrix], trainable = False)(words_input)\n",
    "words = Flatten()(words)\n",
    "\n",
    "cases_input = Input(shape = (7,), dtype = 'int32', name = 'cases_input')\n",
    "cases = Embedding(input_dim = caseEmbeddings.shape[0],\n",
    "                  output_dim = caseEmbeddings.shape[1],\n",
    "                  weights = [caseEmbeddings],\n",
    "                  trainable = False)(cases_input)\n",
    "cases = Flatten()(cases)\n",
    "\n",
    "output = concatenate([words, cases])\n",
    "output = Dense(units = 100, activation = 'tanh')(output)\n",
    "output = Dense(units = len(POS_set), activation = 'softmax')(output)\n",
    "\n",
    "model = Model(inputs =[words_input, cases_input], outputs = [output])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:28.836606Z",
     "start_time": "2017-11-06T06:47:06.061793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.2957\n",
      "Dev-Accuracy: 93.65\n",
      "Test-Accuracy: 93.57\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1853\n",
      "Dev-Accuracy: 94.42\n",
      "Test-Accuracy: 94.24\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1661\n",
      "Dev-Accuracy: 94.67\n",
      "Test-Accuracy: 94.65\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1548\n",
      "Dev-Accuracy: 94.69\n",
      "Test-Accuracy: 94.60\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1475\n",
      "Dev-Accuracy: 94.75\n",
      "Test-Accuracy: 94.73\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1421\n",
      "Dev-Accuracy: 94.76\n",
      "Test-Accuracy: 94.76\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1370\n",
      "Dev-Accuracy: 94.84\n",
      "Test-Accuracy: 94.62\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1331\n",
      "Dev-Accuracy: 94.84\n",
      "Test-Accuracy: 94.77\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1293\n",
      "Dev-Accuracy: 94.91\n",
      "Test-Accuracy: 94.83\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      "38s - loss: 0.1265\n",
      "Dev-Accuracy: 94.98\n",
      "Test-Accuracy: 94.89\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "minibatch_size = 128\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "\n",
    "def predict_classes(prediction):\n",
    " return prediction.argmax(axis=-1)\n",
    " \n",
    "for epoch in range(number_of_epochs):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit([train_set[0], train_set[1]],train_set[2], epochs=1,\n",
    "              batch_size=minibatch_size, verbose=2, shuffle=True)   \n",
    "    \n",
    "    #Predict labels for development set\n",
    "    dev_pred = predict_classes(model.predict([dev_set[0], dev_set[1]]))\n",
    "    dev_acc = np.sum(dev_pred == dev_set[2]) / float(len(dev_set[2]))\n",
    "    print(\"Dev-Accuracy: %.2f\" % (dev_acc*100))\n",
    "    \n",
    "    #Predict labels for test set\n",
    "    test_pred = predict_classes(model.predict([test_set[0], test_set[1]]))\n",
    "    test_acc = np.sum(test_pred == test_set[2]) / float(len(test_set[2]))\n",
    "    print(\"Test-Accuracy: %.2f\" % (test_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "778px",
    "left": "2px",
    "right": "20px",
    "top": "139px",
    "width": "198px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
