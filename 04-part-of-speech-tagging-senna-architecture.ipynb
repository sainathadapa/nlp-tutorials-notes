{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Read-input-data\" data-toc-modified-id=\"Read-input-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Read input data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Find-set-of-POSs-and-unique-words\" data-toc-modified-id=\"Find-set-of-POSs-and-unique-words-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Find set of POSs and unique words</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#glove\" data-toc-modified-id=\"glove-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>glove</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Create-context-index-matrices\" data-toc-modified-id=\"Create-context-index-matrices-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Create context index matrices</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Create-the-model\" data-toc-modified-id=\"Create-the-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Create the model</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Train\" data-toc-modified-id=\"Train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/04-part-of-speech-tagging-senna-architecture.ipynb#Add-case-information\" data-toc-modified-id=\"Add-case-information-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Add case information</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:37.488453Z",
     "start_time": "2017-10-15T15:33:37.372230Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:37.519362Z",
     "start_time": "2017-10-15T15:33:37.490673Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0 or line[0] == '#':\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split('\\t')\n",
    "        sentence.append([splits[0], splits[1]])\n",
    "    \n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:40.380918Z",
     "start_time": "2017-10-15T15:33:37.521878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pos-tagging-senna/train.txt 50545 sentences\n",
      "./pos-tagging-senna/dev.txt 2506 sentences\n",
      "./pos-tagging-senna/test.txt 4134 sentences\n"
     ]
    }
   ],
   "source": [
    "trainSentences = readFile('./pos-tagging-senna/train.txt')\n",
    "devSentences = readFile('./pos-tagging-senna/dev.txt')\n",
    "testSentences = readFile('./pos-tagging-senna/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:40.402429Z",
     "start_time": "2017-10-15T15:33:40.383486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Clearly', 'RB'],\n",
       " [',', 'pct'],\n",
       " ['this', 'DT'],\n",
       " ['was', 'BEDZ'],\n",
       " ['a', 'AT'],\n",
       " ['family', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['crisis', 'NN'],\n",
       " ['.', 'pct']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find set of POSs and unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:41.197714Z",
     "start_time": "2017-10-15T15:33:40.408006Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_set = set()\n",
    "unique_words = set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, label in sentence:\n",
    "            POS_set.add(label)\n",
    "            unique_words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:41.203923Z",
     "start_time": "2017-10-15T15:33:41.200046Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_index_dict = dict([(x,i) for i,x in enumerate(POS_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:41.295976Z",
     "start_time": "2017-10-15T15:33:41.206578Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words.add('__PADDING__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:41.423875Z",
     "start_time": "2017-10-15T15:33:41.299916Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:56.775682Z",
     "start_time": "2017-10-15T15:33:41.425842Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_index = {}\n",
    "f = open('./glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:56.878668Z",
     "start_time": "2017-10-15T15:33:56.777887Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(unique_words), embedding_dim))\n",
    "for word, i in word_index_dict.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:56.909386Z",
     "start_time": "2017-10-15T15:33:56.881971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.66180001e-02,   2.38580001e-03,   1.01259999e-01, ...,\n",
       "         -9.05040026e-01,  -2.65830010e-01,   3.94039989e-01],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  5.73249996e-01,  -4.24730003e-01,  -3.61350000e-01, ...,\n",
       "         -9.43050027e-01,   5.49830019e-01,  -4.99899983e-02],\n",
       "       ..., \n",
       "       [ -9.46839992e-03,   7.38589987e-02,  -2.99039990e-01, ...,\n",
       "         -1.46990001e-01,   4.53269988e-01,   1.27079999e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [ -1.22570001e-01,  -5.88829994e-01,   3.74179989e-01, ...,\n",
       "         -7.95229978e-04,  -2.82409996e-01,   5.66399992e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:57.082563Z",
     "start_time": "2017-10-15T15:33:56.911695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49816, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create context index matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:57.251761Z",
     "start_time": "2017-10-15T15:33:57.087408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Clearly', 'RB'],\n",
       " [',', 'pct'],\n",
       " ['this', 'DT'],\n",
       " ['was', 'BEDZ'],\n",
       " ['a', 'AT'],\n",
       " ['family', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['crisis', 'NN'],\n",
       " ['.', 'pct']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:33:57.375169Z",
     "start_time": "2017-10-15T15:33:57.255209Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    xMatrix = []\n",
    "    yVector = []\n",
    "    \n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            wordIndices = []\n",
    "            for pos in range(i-windowsize, i+windowsize+1):\n",
    "                if pos < 0 or pos >= len(sentence):\n",
    "                    wordIndices.append(padding_index)\n",
    "                else:\n",
    "                    word = sentence[pos][0]\n",
    "                    if word.lower() in word_index_dict:\n",
    "                        wordIndices.append(word_index_dict[word.lower()])\n",
    "                    else:\n",
    "                        wordIndices.append(padding_index)\n",
    "            \n",
    "            yVector.append(POS_index_dict[sentence[i][1]])\n",
    "            xMatrix.append(wordIndices)\n",
    "    \n",
    "    return (np.asarray(xMatrix), np.asarray(yVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:06.839389Z",
     "start_time": "2017-10-15T15:33:57.378735Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:06.857784Z",
     "start_time": "2017-10-15T15:34:06.853791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026265, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:07.079147Z",
     "start_time": "2017-10-15T15:34:06.859823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026265,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:08.357915Z",
     "start_time": "2017-10-15T15:34:07.083749Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_set  = createMatrices(devSentences, windowsize=3)\n",
    "test_set = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:24.455396Z",
     "start_time": "2017-10-15T15:34:08.364375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:24.565211Z",
     "start_time": "2017-10-15T15:34:24.458452Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape = (7,), dtype = 'int32', name = 'words_input')\n",
    "words = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                  weights = [embedding_matrix], trainable = False)(words_input)\n",
    "words = Flatten()(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:24.727204Z",
     "start_time": "2017-10-15T15:34:24.568027Z"
    }
   },
   "outputs": [],
   "source": [
    "output = Dense(units = 100, activation = 'tanh')(words)\n",
    "output = Dense(units = len(POS_set), activation = 'softmax')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:24.820649Z",
     "start_time": "2017-10-15T15:34:24.729438Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(model = Model(inputs = words_input, outputs = [output])inputs = words_input, outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:34:24.948128Z",
     "start_time": "2017-10-15T15:34:24.823885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "words_input (InputLayer)     (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 7, 100)            4981600   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               70100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                8484      \n",
      "=================================================================\n",
      "Total params: 5,060,184\n",
      "Trainable params: 78,584\n",
      "Non-trainable params: 4,981,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:28:18.153266Z",
     "start_time": "2017-10-15T15:28:18.150069Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:42:56.618701Z",
     "start_time": "2017-10-15T15:34:24.950339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      "44s - loss: 0.3227\n",
      "Dev-Accuracy: 93.26\n",
      "Test-Accuracy: 93.17\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      "44s - loss: 0.2078\n",
      "Dev-Accuracy: 93.76\n",
      "Test-Accuracy: 93.56\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      "46s - loss: 0.1871\n",
      "Dev-Accuracy: 94.06\n",
      "Test-Accuracy: 93.93\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      "46s - loss: 0.1758\n",
      "Dev-Accuracy: 94.20\n",
      "Test-Accuracy: 94.18\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      "47s - loss: 0.1680\n",
      "Dev-Accuracy: 94.15\n",
      "Test-Accuracy: 94.13\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      "47s - loss: 0.1611\n",
      "Dev-Accuracy: 94.32\n",
      "Test-Accuracy: 94.26\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      "46s - loss: 0.1569\n",
      "Dev-Accuracy: 94.29\n",
      "Test-Accuracy: 94.29\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      "45s - loss: 0.1527\n",
      "Dev-Accuracy: 94.43\n",
      "Test-Accuracy: 94.38\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      "46s - loss: 0.1494\n",
      "Dev-Accuracy: 94.30\n",
      "Test-Accuracy: 94.27\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      "45s - loss: 0.1458\n",
      "Dev-Accuracy: 94.38\n",
      "Test-Accuracy: 94.34\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "minibatch_size = 128\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "\n",
    "def predict_classes(prediction):\n",
    " return prediction.argmax(axis=-1)\n",
    " \n",
    "for epoch in range(number_of_epochs):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit(train_set[0], train_set[1], epochs=1, batch_size=minibatch_size, verbose=2, shuffle=True)   \n",
    "    \n",
    "    #Predict labels for development set\n",
    "    dev_pred = predict_classes(model.predict(dev_set[0]))\n",
    "    dev_acc = np.sum(dev_pred == dev_set[1]) / float(len(dev_set[1]))\n",
    "    print(\"Dev-Accuracy: %.2f\" % (dev_acc*100))\n",
    "    \n",
    "    #Predict labels for test set\n",
    "    test_pred = predict_classes(model.predict(test_set[0]))\n",
    "    test_acc = np.sum(test_pred == test_set[1]) / float(len(test_set[1]))\n",
    "    print(\"Test-Accuracy: %.2f\" % (test_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add case information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:49:54.780043Z",
     "start_time": "2017-10-15T15:49:54.761740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_index_dict = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4,\n",
    "                   'mainly_numeric':5, 'contains_digit': 6, '__PADDING__':7}\n",
    "caseEmbeddings = np.identity(len(case_index_dict), dtype='float32')\n",
    "caseEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:49:57.210986Z",
     "start_time": "2017-10-15T15:49:57.176908Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCase(word):\n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "    \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "    return case_index_dict[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:53:29.617898Z",
     "start_time": "2017-10-15T15:53:29.555041Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    xMatrix    = []\n",
    "    caseMatrix = []\n",
    "    yVector    = []\n",
    "    \n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            wordIndices = []\n",
    "            caseIndices = []\n",
    "            for pos in range(i-windowsize, i+windowsize+1):\n",
    "                if pos < 0 or pos >= len(sentence):\n",
    "                    wordIndices.append(padding_index)\n",
    "                    caseIndices.append(case_index_dict['__PADDING__'])\n",
    "                else:\n",
    "                    word = sentence[pos][0]\n",
    "                    if word.lower() in word_index_dict:\n",
    "                        wordIndices.append(word_index_dict[word.lower()])\n",
    "                    else:\n",
    "                        wordIndices.append(padding_index)\n",
    "                    caseIndices.append(getCase(word))\n",
    "            \n",
    "            yVector.append(POS_index_dict[sentence[i][1]])\n",
    "            xMatrix.append(wordIndices)\n",
    "            caseMatrix.append(caseIndices)\n",
    "    \n",
    "    return (np.asarray(xMatrix), np.asarray(caseMatrix), np.asarray(yVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:53:58.348864Z",
     "start_time": "2017-10-15T15:53:29.977831Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)\n",
    "dev_set  = createMatrices(devSentences, windowsize=3)\n",
    "test_set = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:54:09.079595Z",
     "start_time": "2017-10-15T15:54:09.072178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1026265, 7)\n",
      "(1026265, 7)\n",
      "(1026265,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0].shape)\n",
    "print(train_set[1].shape)\n",
    "print(train_set[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:58:09.318348Z",
     "start_time": "2017-10-15T15:58:09.095773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, 7)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "cases_input (InputLayer)         (None, 7)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 7, 100)        4981600     words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 7, 8)          64          cases_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 700)           0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 56)            0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 756)           0           flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 100)           75700       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 84)            8484        dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 5,065,848\n",
      "Trainable params: 84,184\n",
      "Non-trainable params: 4,981,664\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape = (7,), dtype = 'int32', name = 'words_input')\n",
    "words = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                  weights = [embedding_matrix], trainable = False)(words_input)\n",
    "words = Flatten()(words)\n",
    "\n",
    "cases_input = Input(shape = (7,), dtype = 'int32', name = 'cases_input')\n",
    "cases = Embedding(input_dim = caseEmbeddings.shape[0],\n",
    "                  output_dim = caseEmbeddings.shape[1],\n",
    "                  weights = [caseEmbeddings],\n",
    "                  trainable = False)(cases_input)\n",
    "cases = Flatten()(cases)\n",
    "\n",
    "output = concatenate([words, cases])\n",
    "output = Dense(units = 100, activation = 'tanh')(output)\n",
    "output = Dense(units = len(POS_set), activation = 'softmax')(output)\n",
    "\n",
    "model = Model(inputs =[words_input, cases_input], outputs = [output])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:09:00.334351Z",
     "start_time": "2017-10-15T15:59:54.011945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      "49s - loss: 0.2956\n",
      "Dev-Accuracy: 93.73\n",
      "Test-Accuracy: 93.70\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      "49s - loss: 0.1848\n",
      "Dev-Accuracy: 94.22\n",
      "Test-Accuracy: 94.33\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      "48s - loss: 0.1654\n",
      "Dev-Accuracy: 94.72\n",
      "Test-Accuracy: 94.64\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      "48s - loss: 0.1540\n",
      "Dev-Accuracy: 94.57\n",
      "Test-Accuracy: 94.48\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      "48s - loss: 0.1466\n",
      "Dev-Accuracy: 94.98\n",
      "Test-Accuracy: 94.77\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      "48s - loss: 0.1409\n",
      "Dev-Accuracy: 94.76\n",
      "Test-Accuracy: 94.71\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      "48s - loss: 0.1362\n",
      "Dev-Accuracy: 94.98\n",
      "Test-Accuracy: 94.91\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      "49s - loss: 0.1321\n",
      "Dev-Accuracy: 94.86\n",
      "Test-Accuracy: 94.67\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      "48s - loss: 0.1287\n",
      "Dev-Accuracy: 94.98\n",
      "Test-Accuracy: 94.86\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      "48s - loss: 0.1257\n",
      "Dev-Accuracy: 95.03\n",
      "Test-Accuracy: 94.79\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "minibatch_size = 128\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "\n",
    "def predict_classes(prediction):\n",
    " return prediction.argmax(axis=-1)\n",
    " \n",
    "for epoch in range(number_of_epochs):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit([train_set[0], train_set[1]],train_set[2], epochs=1,\n",
    "              batch_size=minibatch_size, verbose=2, shuffle=True)   \n",
    "    \n",
    "    #Predict labels for development set\n",
    "    dev_pred = predict_classes(model.predict([dev_set[0], dev_set[1]]))\n",
    "    dev_acc = np.sum(dev_pred == dev_set[2]) / float(len(dev_set[2]))\n",
    "    print(\"Dev-Accuracy: %.2f\" % (dev_acc*100))\n",
    "    \n",
    "    #Predict labels for test set\n",
    "    test_pred = predict_classes(model.predict([test_set[0], test_set[1]]))\n",
    "    test_acc = np.sum(test_pred == test_set[2]) / float(len(test_set[2]))\n",
    "    print(\"Test-Accuracy: %.2f\" % (test_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "778px",
    "left": "3px",
    "right": "20px",
    "top": "138px",
    "width": "198px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
