{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-input-data\" data-toc-modified-id=\"Read-input-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Read input data</a></span></li><li><span><a href=\"#Find-set-of-POSs-and-unique-words\" data-toc-modified-id=\"Find-set-of-POSs-and-unique-words-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Find set of POSs and unique words</a></span></li><li><span><a href=\"#glove\" data-toc-modified-id=\"glove-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>glove</a></span></li><li><span><a href=\"#Create-context-index-matrices\" data-toc-modified-id=\"Create-context-index-matrices-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Create context index matrices</a></span></li></ul></li><li><span><a href=\"#Create-the-model\" data-toc-modified-id=\"Create-the-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Create the model</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"#Add-case-information\" data-toc-modified-id=\"Add-case-information-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Add case information</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/UKPLab/deeplearning4nlp-tutorial/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:25.070482Z",
     "start_time": "2018-04-25T15:55:24.989280Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:25.082980Z",
     "start_time": "2018-04-25T15:55:25.073639Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0 or line[0] == '#':\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split('\\t')\n",
    "        sentence.append([splits[0], splits[1]])\n",
    "    \n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:26.416763Z",
     "start_time": "2018-04-25T15:55:25.084366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pos-tagging-senna/train.txt 50545 sentences\n",
      "data/pos-tagging-senna/dev.txt 2506 sentences\n",
      "data/pos-tagging-senna/test.txt 4134 sentences\n"
     ]
    }
   ],
   "source": [
    "trainSentences = readFile('data/pos-tagging-senna/train.txt')\n",
    "devSentences = readFile('data/pos-tagging-senna/dev.txt')\n",
    "testSentences = readFile('data/pos-tagging-senna/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:26.425909Z",
     "start_time": "2018-04-25T15:55:26.418271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Clearly', 'RB'],\n",
       " [',', 'pct'],\n",
       " ['this', 'DT'],\n",
       " ['was', 'BEDZ'],\n",
       " ['a', 'AT'],\n",
       " ['family', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['crisis', 'NN'],\n",
       " ['.', 'pct']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find set of POSs and unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:26.935161Z",
     "start_time": "2018-04-25T15:55:26.427675Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_set = set()\n",
    "unique_words = set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, label in sentence:\n",
    "            POS_set.add(label)\n",
    "            unique_words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:26.938882Z",
     "start_time": "2018-04-25T15:55:26.936471Z"
    }
   },
   "outputs": [],
   "source": [
    "POS_index_dict = dict([(x,i) for i,x in enumerate(POS_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:27.057750Z",
     "start_time": "2018-04-25T15:55:26.940103Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words.add('__PADDING__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:27.182866Z",
     "start_time": "2018-04-25T15:55:27.062134Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:35.503361Z",
     "start_time": "2018-04-25T15:55:27.184596Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_index = {}\n",
    "f = open('data/glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:35.569333Z",
     "start_time": "2018-04-25T15:55:35.504911Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(unique_words), embedding_dim))\n",
    "for word, i in word_index_dict.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:35.646611Z",
     "start_time": "2018-04-25T15:55:35.570924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.016446  , -0.16006   , -0.35425001, ...,  0.33123001,\n",
       "         0.40114999, -0.13247   ],\n",
       "       [-0.20118   , -0.65382999, -0.061251  , ..., -0.34241   ,\n",
       "        -0.91162997,  0.071792  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.24512   ,  0.23903   , -0.85452002, ...,  0.12566   ,\n",
       "         0.45653999,  0.27656999],\n",
       "       [-0.50099999,  0.41269001,  0.082855  , ..., -0.98444003,\n",
       "         0.58904999,  0.17519   ],\n",
       "       [-0.21916001,  0.1708    ,  0.16096   , ..., -0.13861001,\n",
       "         0.014642  , -0.38846001]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:35.727169Z",
     "start_time": "2018-04-25T15:55:35.650386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49816, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create context index matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:35.808687Z",
     "start_time": "2018-04-25T15:55:35.730970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Clearly', 'RB'],\n",
       " [',', 'pct'],\n",
       " ['this', 'DT'],\n",
       " ['was', 'BEDZ'],\n",
       " ['a', 'AT'],\n",
       " ['family', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['crisis', 'NN'],\n",
       " ['.', 'pct']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:35.910100Z",
     "start_time": "2018-04-25T15:55:35.809927Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    xMatrix = []\n",
    "    yVector = []\n",
    "    \n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            wordIndices = []\n",
    "            for pos in range(i-windowsize, i+windowsize+1):\n",
    "                if pos < 0 or pos >= len(sentence):\n",
    "                    wordIndices.append(padding_index)\n",
    "                else:\n",
    "                    word = sentence[pos][0]\n",
    "                    if word.lower() in word_index_dict:\n",
    "                        wordIndices.append(word_index_dict[word.lower()])\n",
    "                    else:\n",
    "                        wordIndices.append(padding_index)\n",
    "            \n",
    "            yVector.append(POS_index_dict[sentence[i][1]])\n",
    "            xMatrix.append(wordIndices)\n",
    "    \n",
    "    return (np.asarray(xMatrix), np.asarray(yVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:40.511961Z",
     "start_time": "2018-04-25T15:55:35.913589Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:40.517052Z",
     "start_time": "2018-04-25T15:55:40.513542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026265, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:40.610830Z",
     "start_time": "2018-04-25T15:55:40.518787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026265,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:41.264307Z",
     "start_time": "2018-04-25T15:55:40.614680Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_set  = createMatrices(devSentences, windowsize=3)\n",
    "test_set = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:42.121700Z",
     "start_time": "2018-04-25T15:55:41.265456Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/code/personal/nlp-tutorials-notes/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:42.202396Z",
     "start_time": "2018-04-25T15:55:42.123063Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape = (7,), dtype = 'int32', name = 'words_input')\n",
    "words = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                  weights = [embedding_matrix], trainable = False)(words_input)\n",
    "words = Flatten()(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:42.263990Z",
     "start_time": "2018-04-25T15:55:42.204069Z"
    }
   },
   "outputs": [],
   "source": [
    "output = Dense(units = 100, activation = 'tanh')(words)\n",
    "output = Dense(units = len(POS_set), activation = 'softmax')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:42.568963Z",
     "start_time": "2018-04-25T15:55:42.265690Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs = words_input, outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:55:42.820435Z",
     "start_time": "2018-04-25T15:55:42.572694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "words_input (InputLayer)     (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 7, 100)            4981600   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               70100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                8484      \n",
      "=================================================================\n",
      "Total params: 5,060,184\n",
      "Trainable params: 78,584\n",
      "Non-trainable params: 4,981,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T15:28:18.153266Z",
     "start_time": "2017-10-15T15:28:18.150069Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:59:12.478318Z",
     "start_time": "2018-04-25T15:55:42.821845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.3203\n",
      "Dev-Accuracy: 93.25\n",
      "Test-Accuracy: 93.20\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.2034\n",
      "Dev-Accuracy: 93.99\n",
      "Test-Accuracy: 93.97\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1827\n",
      "Dev-Accuracy: 94.02\n",
      "Test-Accuracy: 93.93\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1716\n",
      "Dev-Accuracy: 94.42\n",
      "Test-Accuracy: 94.16\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1633\n",
      "Dev-Accuracy: 94.45\n",
      "Test-Accuracy: 94.27\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1573\n",
      "Dev-Accuracy: 94.35\n",
      "Test-Accuracy: 94.27\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1525\n",
      "Dev-Accuracy: 94.55\n",
      "Test-Accuracy: 94.44\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1487\n",
      "Dev-Accuracy: 94.57\n",
      "Test-Accuracy: 94.40\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1451\n",
      "Dev-Accuracy: 94.56\n",
      "Test-Accuracy: 94.48\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      " - 19s - loss: 0.1419\n",
      "Dev-Accuracy: 94.64\n",
      "Test-Accuracy: 94.56\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "minibatch_size = 128\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "\n",
    "def predict_classes(prediction):\n",
    " return prediction.argmax(axis=-1)\n",
    " \n",
    "for epoch in range(number_of_epochs):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit(train_set[0], train_set[1], epochs=1, batch_size=minibatch_size, verbose=2, shuffle=True)   \n",
    "    \n",
    "    #Predict labels for development set\n",
    "    dev_pred = predict_classes(model.predict(dev_set[0]))\n",
    "    dev_acc = np.sum(dev_pred == dev_set[1]) / float(len(dev_set[1]))\n",
    "    print(\"Dev-Accuracy: %.2f\" % (dev_acc*100))\n",
    "    \n",
    "    #Predict labels for test set\n",
    "    test_pred = predict_classes(model.predict(test_set[0]))\n",
    "    test_acc = np.sum(test_pred == test_set[1]) / float(len(test_set[1]))\n",
    "    print(\"Test-Accuracy: %.2f\" % (test_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add case information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:59:12.485312Z",
     "start_time": "2018-04-25T15:59:12.480036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_index_dict = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4,\n",
    "                   'mainly_numeric':5, 'contains_digit': 6, '__PADDING__':7}\n",
    "caseEmbeddings = np.identity(len(case_index_dict), dtype='float32')\n",
    "caseEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:59:12.610896Z",
     "start_time": "2018-04-25T15:59:12.486865Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCase(word):\n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "    \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "    return case_index_dict[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:59:12.730751Z",
     "start_time": "2018-04-25T15:59:12.613987Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, windowsize):\n",
    "    xMatrix    = []\n",
    "    caseMatrix = []\n",
    "    yVector    = []\n",
    "    \n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            wordIndices = []\n",
    "            caseIndices = []\n",
    "            for pos in range(i-windowsize, i+windowsize+1):\n",
    "                if pos < 0 or pos >= len(sentence):\n",
    "                    wordIndices.append(padding_index)\n",
    "                    caseIndices.append(case_index_dict['__PADDING__'])\n",
    "                else:\n",
    "                    word = sentence[pos][0]\n",
    "                    if word.lower() in word_index_dict:\n",
    "                        wordIndices.append(word_index_dict[word.lower()])\n",
    "                    else:\n",
    "                        wordIndices.append(padding_index)\n",
    "                    caseIndices.append(getCase(word))\n",
    "            \n",
    "            yVector.append(POS_index_dict[sentence[i][1]])\n",
    "            xMatrix.append(wordIndices)\n",
    "            caseMatrix.append(caseIndices)\n",
    "    \n",
    "    return (np.asarray(xMatrix), np.asarray(caseMatrix), np.asarray(yVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:59:26.799418Z",
     "start_time": "2018-04-25T15:59:12.734419Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = createMatrices(trainSentences, windowsize=3)\n",
    "dev_set  = createMatrices(devSentences, windowsize=3)\n",
    "test_set = createMatrices(testSentences, windowsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:59:26.803718Z",
     "start_time": "2018-04-25T15:59:26.800943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1026265, 7)\n",
      "(1026265, 7)\n",
      "(1026265,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0].shape)\n",
    "print(train_set[1].shape)\n",
    "print(train_set[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T15:59:27.042047Z",
     "start_time": "2018-04-25T15:59:26.805258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_input (InputLayer)        (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cases_input (InputLayer)        (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 7, 100)       4981600     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 7, 8)         64          cases_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 700)          0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 56)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 756)          0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          75700       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 84)           8484        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,065,848\n",
      "Trainable params: 84,184\n",
      "Non-trainable params: 4,981,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape = (7,), dtype = 'int32', name = 'words_input')\n",
    "words = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                  weights = [embedding_matrix], trainable = False)(words_input)\n",
    "words = Flatten()(words)\n",
    "\n",
    "cases_input = Input(shape = (7,), dtype = 'int32', name = 'cases_input')\n",
    "cases = Embedding(input_dim = caseEmbeddings.shape[0],\n",
    "                  output_dim = caseEmbeddings.shape[1],\n",
    "                  weights = [caseEmbeddings],\n",
    "                  trainable = False)(cases_input)\n",
    "cases = Flatten()(cases)\n",
    "\n",
    "output = concatenate([words, cases])\n",
    "output = Dense(units = 100, activation = 'tanh')(output)\n",
    "output = Dense(units = len(POS_set), activation = 'softmax')(output)\n",
    "\n",
    "model = Model(inputs =[words_input, cases_input], outputs = [output])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:03:22.720921Z",
     "start_time": "2018-04-25T15:59:27.044767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.2942\n",
      "Dev-Accuracy: 93.50\n",
      "Test-Accuracy: 93.54\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1835\n",
      "Dev-Accuracy: 94.27\n",
      "Test-Accuracy: 94.27\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1641\n",
      "Dev-Accuracy: 94.62\n",
      "Test-Accuracy: 94.51\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1531\n",
      "Dev-Accuracy: 94.58\n",
      "Test-Accuracy: 94.70\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1456\n",
      "Dev-Accuracy: 94.53\n",
      "Test-Accuracy: 94.50\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1401\n",
      "Dev-Accuracy: 94.86\n",
      "Test-Accuracy: 94.81\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1352\n",
      "Dev-Accuracy: 94.96\n",
      "Test-Accuracy: 94.71\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1311\n",
      "Dev-Accuracy: 94.93\n",
      "Test-Accuracy: 94.79\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1284\n",
      "Dev-Accuracy: 94.87\n",
      "Test-Accuracy: 94.75\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      " - 20s - loss: 0.1253\n",
      "Dev-Accuracy: 94.74\n",
      "Test-Accuracy: 94.60\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "minibatch_size = 128\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "\n",
    "def predict_classes(prediction):\n",
    " return prediction.argmax(axis=-1)\n",
    " \n",
    "for epoch in range(number_of_epochs):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit([train_set[0], train_set[1]],train_set[2], epochs=1,\n",
    "              batch_size=minibatch_size, verbose=2, shuffle=True)   \n",
    "    \n",
    "    #Predict labels for development set\n",
    "    dev_pred = predict_classes(model.predict([dev_set[0], dev_set[1]]))\n",
    "    dev_acc = np.sum(dev_pred == dev_set[2]) / float(len(dev_set[2]))\n",
    "    print(\"Dev-Accuracy: %.2f\" % (dev_acc*100))\n",
    "    \n",
    "    #Predict labels for test set\n",
    "    test_pred = predict_classes(model.predict([test_set[0], test_set[1]]))\n",
    "    test_acc = np.sum(test_pred == test_set[2]) / float(len(test_set[2]))\n",
    "    print(\"Test-Accuracy: %.2f\" % (test_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "778px",
    "left": "2px",
    "right": "20px",
    "top": "139px",
    "width": "198px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
