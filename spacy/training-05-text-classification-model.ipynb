{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/explosion/spaCy/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a convolutional neural network text classifier on the\n",
    "IMDB dataset, using the TextCategorizer component. The dataset will be loaded\n",
    "automatically via Thinc's built-in dataset loader. The model is added to\n",
    "spacy.pipeline, and predictions are available via `doc.cats`. For more details,\n",
    "see the documentation:\n",
    "\n",
    "https://spacy.io/usage/training#section-textcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextCategorizer**\n",
    "\n",
    "Add text categorization models to spaCy pipelines.\n",
    "\n",
    "The model supports classification with multiple, non-mutually exclusive labels. You can change the model architecture rather easily, but by default, the TextCategorizer class uses a convolutional neural network to assign position-sensitive vectors to each word in the document. The TextCategorizer uses its own CNN model, to avoid sharing weights with the other pipeline components. The document tensor is then summarized by concatenating max and mean pooling, and a multilayer perceptron is used to predict an output vector of length nr_class, before a logistic activation is applied elementwise. The value of each output neuron is the probability that some class is present.\n",
    "\n",
    "This class is a subclass of Pipe  and follows the same API. The pipeline component is available in the processing pipeline via the ID \"textcat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T10:44:25.887197Z",
     "start_time": "2018-04-30T10:44:25.634649Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import thinc.extra.datasets\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T10:44:46.686002Z",
     "start_time": "2018-04-30T10:44:46.546936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create blank language class\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T10:44:57.218398Z",
     "start_time": "2018-04-30T10:44:57.211317Z"
    }
   },
   "outputs": [],
   "source": [
    "# add the text classifier to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T10:45:04.520348Z",
     "start_time": "2018-04-30T10:45:04.510500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add label to text classifier\n",
    "textcat.add_label('POSITIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T10:45:28.678160Z",
     "start_time": "2018-04-30T10:45:28.667514Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(limit=0, split=0.8):\n",
    "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
    "    # Partition off part of the train data for evaluation\n",
    "    train_data, _ = thinc.extra.datasets.imdb()\n",
    "    random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y)} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T11:13:32.581510Z",
     "start_time": "2018-04-30T11:13:31.105482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB data...\n",
      "Using 2000 examples (1600 training, 400 evaluation)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB data...\")\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=2000)\n",
    "print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "      .format(2000, len(train_texts), len(dev_texts)))\n",
    "train_data = list(zip(train_texts,\n",
    "                      [{'cats': cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T11:16:38.519717Z",
     "start_time": "2018-04-30T11:16:38.498960Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T11:31:25.445919Z",
     "start_time": "2018-04-30T11:16:39.037769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "42.831\t0.794\t0.856\t0.824\n",
      "19.204\t0.798\t0.876\t0.835\n",
      "7.980\t0.813\t0.876\t0.844\n",
      "4.348\t0.825\t0.876\t0.850\n",
      "2.531\t0.825\t0.876\t0.850\n",
      "1.602\t0.823\t0.861\t0.841\n",
      "1.217\t0.811\t0.861\t0.835\n",
      "0.747\t0.822\t0.856\t0.838\n",
      "0.312\t0.823\t0.861\t0.841\n",
      "0.605\t0.824\t0.866\t0.844\n",
      "0.463\t0.819\t0.840\t0.830\n",
      "0.255\t0.828\t0.845\t0.837\n",
      "0.265\t0.805\t0.851\t0.827\n",
      "0.191\t0.814\t0.856\t0.834\n",
      "0.312\t0.813\t0.851\t0.831\n",
      "0.117\t0.817\t0.851\t0.833\n",
      "0.224\t0.812\t0.845\t0.828\n",
      "0.293\t0.823\t0.840\t0.832\n",
      "0.230\t0.788\t0.845\t0.816\n",
      "0.849\t0.799\t0.840\t0.819\n"
     ]
    }
   ],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "    for i in range(20):\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "              .format(losses['textcat'], scores['textcat_p'],\n",
    "                      scores['textcat_r'], scores['textcat_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T11:31:25.453231Z",
     "start_time": "2018-04-30T11:31:25.447630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie sucked {'POSITIVE': 0.2625103294849396}\n"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "test_text = \"This movie sucked\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
