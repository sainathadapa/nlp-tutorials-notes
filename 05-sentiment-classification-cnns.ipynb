{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/UKPLab/deeplearning4nlp-tutorial/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:49.817436Z",
     "start_time": "2018-04-25T16:06:49.721742Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:49.826681Z",
     "start_time": "2018-04-25T16:06:49.820501Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "    \n",
    "    print(filepath, len(sentences), 'sentences')\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:50.033984Z",
     "start_time": "2018-04-25T16:06:49.829119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sent-class-cnn/train.txt 5330 sentences\n",
      "data/sent-class-cnn/dev.txt 2664 sentences\n",
      "data/sent-class-cnn/test.txt 2668 sentences\n"
     ]
    }
   ],
   "source": [
    "traind = readFile('data/sent-class-cnn/train.txt')\n",
    "devd   = readFile('data/sent-class-cnn/dev.txt')\n",
    "testd  = readFile('data/sent-class-cnn/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:50.103847Z",
     "start_time": "2018-04-25T16:06:50.038581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'like',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'movies',\n",
       "  'with',\n",
       "  'more',\n",
       "  'elves',\n",
       "  'and',\n",
       "  'snow',\n",
       "  'and',\n",
       "  'less',\n",
       "  'pimps',\n",
       "  'and',\n",
       "  \"ho's\",\n",
       "  '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind[0][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:50.217547Z",
     "start_time": "2018-04-25T16:06:50.108945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind[1][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:50.367291Z",
     "start_time": "2018-04-25T16:06:50.222789Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "for dataset in [traind, devd, testd]:\n",
    "    for sentence in dataset[0]:\n",
    "        for word in sentence:\n",
    "            unique_words.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:50.399097Z",
     "start_time": "2018-04-25T16:06:50.368528Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words.add('__PADDING__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:50.508361Z",
     "start_time": "2018-04-25T16:06:50.400490Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:58.916962Z",
     "start_time": "2018-04-25T16:06:50.513004Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_index = {}\n",
    "f = open('data/glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:58.947868Z",
     "start_time": "2018-04-25T16:06:58.918630Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(unique_words), embedding_dim))\n",
    "for word, i in word_index_dict.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:59.046635Z",
     "start_time": "2018-04-25T16:06:58.949779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21348, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:58:06.598523Z",
     "start_time": "2017-10-15T16:58:06.593711Z"
    }
   },
   "source": [
    "## matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:59.132726Z",
     "start_time": "2018-04-25T16:06:59.048207Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrix(sentences):\n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    xMatrix = []\n",
    "    for sentence in sentences:\n",
    "        wordIndices = []\n",
    "        for word in sentence:\n",
    "            if word.lower() in word_index_dict:\n",
    "                wordIndices.append(word_index_dict[word.lower()])\n",
    "            else:\n",
    "                wordIndices.append(padding_index)\n",
    "        xMatrix.append(wordIndices)\n",
    "    \n",
    "    return xMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:59.368692Z",
     "start_time": "2018-04-25T16:06:59.134425Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mat = createMatrix(traind[0])\n",
    "dev_mat   = createMatrix(devd[0])\n",
    "test_mat  = createMatrix(testd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:59.380269Z",
     "start_time": "2018-04-25T16:06:59.370077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "28\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_mat[0]))\n",
    "print(len(train_mat[1]))\n",
    "print(len(train_mat[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:59.486294Z",
     "start_time": "2018-04-25T16:06:59.381530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_mat + dev_mat + test_mat:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "max_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:06:59.568045Z",
     "start_time": "2018-04-25T16:06:59.491630Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.array(traind[1])\n",
    "y_dev = np.array(devd[1])\n",
    "y_test = np.array(testd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:00.478140Z",
     "start_time": "2018-04-25T16:06:59.570231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/code/personal/nlp-tutorials-notes/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:00.533559Z",
     "start_time": "2018-04-25T16:07:00.479816Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(train_mat, maxlen=max_sentence_len)\n",
    "X_dev = sequence.pad_sequences(dev_mat, maxlen=max_sentence_len)\n",
    "X_test = sequence.pad_sequences(test_mat, maxlen=max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:00.573422Z",
     "start_time": "2018-04-25T16:07:00.535192Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate, Embedding, Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:00.761852Z",
     "start_time": "2018-04-25T16:07:00.574817Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(max_sentence_len,), dtype='int32', name='words_input')\n",
    "wordsEmbeddingLayer = Embedding(embedding_matrix.shape[0],\n",
    "                                embedding_matrix.shape[1],                                     \n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "words = wordsEmbeddingLayer(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:01.010082Z",
     "start_time": "2018-04-25T16:07:00.763322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sai/code/personal/nlp-tutorials-notes/.env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "#Now we add a variable number of convolutions\n",
    "words_convolutions = []\n",
    "for filter_length in [1,2,3]:\n",
    "    words_conv = Convolution1D(filters=50,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "                            \n",
    "    words_conv = GlobalMaxPooling1D()(words_conv)      \n",
    "    \n",
    "    words_convolutions.append(words_conv)  \n",
    "\n",
    "output = concatenate(words_convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:01.014399Z",
     "start_time": "2018-04-25T16:07:01.011870Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:01.223616Z",
     "start_time": "2018-04-25T16:07:01.016289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_input (InputLayer)        (None, 59)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 59, 100)      2134800     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 59, 50)       5050        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 59, 50)       10050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 59, 50)       15050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 50)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 50)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 50)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 150)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          15100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,180,151\n",
      "Trainable params: 45,351\n",
      "Non-trainable params: 2,134,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output = concatenate(words_convolutions)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(100, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "output = Dropout(0.25)(output)\n",
    "output = Dense(1, activation='sigmoid',  kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "model = Model(inputs=[words_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T16:07:35.937914Z",
     "start_time": "2018-04-25T16:07:01.227519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      " - 2s - loss: 1.6136 - acc: 0.5257\n",
      "Dev-Accuracy: 60.06% (loss: 1.2605)\n",
      "Test-Accuracy: 60.68% (loss: 1.2585)\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.1076 - acc: 0.6242\n",
      "Dev-Accuracy: 71.21% (loss: 0.9118)\n",
      "Test-Accuracy: 70.50% (loss: 0.9144)\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.8445 - acc: 0.7011\n",
      "Dev-Accuracy: 72.56% (loss: 0.7524)\n",
      "Test-Accuracy: 71.78% (loss: 0.7575)\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.7087 - acc: 0.7319\n",
      "Dev-Accuracy: 72.18% (loss: 0.6717)\n",
      "Test-Accuracy: 71.70% (loss: 0.6821)\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.6224 - acc: 0.7565\n",
      "Dev-Accuracy: 74.40% (loss: 0.6129)\n",
      "Test-Accuracy: 73.16% (loss: 0.6253)\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.5687 - acc: 0.7705\n",
      "Dev-Accuracy: 74.29% (loss: 0.5836)\n",
      "Test-Accuracy: 73.80% (loss: 0.5974)\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.5228 - acc: 0.7884\n",
      "Dev-Accuracy: 75.30% (loss: 0.5657)\n",
      "Test-Accuracy: 73.88% (loss: 0.5840)\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.4927 - acc: 0.7970\n",
      "Dev-Accuracy: 75.49% (loss: 0.5513)\n",
      "Test-Accuracy: 74.44% (loss: 0.5704)\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.4796 - acc: 0.8077\n",
      "Dev-Accuracy: 75.45% (loss: 0.5476)\n",
      "Test-Accuracy: 74.14% (loss: 0.5659)\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.4457 - acc: 0.8236\n",
      "Dev-Accuracy: 75.23% (loss: 0.5620)\n",
      "Test-Accuracy: 73.80% (loss: 0.5771)\n",
      "\n",
      "------------- Epoch 11 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.4210 - acc: 0.8368\n",
      "Dev-Accuracy: 74.10% (loss: 0.5641)\n",
      "Test-Accuracy: 73.65% (loss: 0.5804)\n",
      "\n",
      "------------- Epoch 12 ------------\n",
      "Epoch 1/1\n",
      " - 2s - loss: 0.4084 - acc: 0.8400\n",
      "Dev-Accuracy: 75.71% (loss: 0.5514)\n",
      "Test-Accuracy: 74.03% (loss: 0.5708)\n",
      "\n",
      "------------- Epoch 13 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3955 - acc: 0.8559\n",
      "Dev-Accuracy: 75.68% (loss: 0.5504)\n",
      "Test-Accuracy: 74.14% (loss: 0.5704)\n",
      "\n",
      "------------- Epoch 14 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3859 - acc: 0.8604\n",
      "Dev-Accuracy: 75.19% (loss: 0.5675)\n",
      "Test-Accuracy: 74.55% (loss: 0.5861)\n",
      "\n",
      "------------- Epoch 15 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3711 - acc: 0.8632\n",
      "Dev-Accuracy: 75.19% (loss: 0.5656)\n",
      "Test-Accuracy: 74.44% (loss: 0.5838)\n",
      "\n",
      "------------- Epoch 16 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3584 - acc: 0.8760\n",
      "Dev-Accuracy: 76.16% (loss: 0.5613)\n",
      "Test-Accuracy: 74.21% (loss: 0.5889)\n",
      "\n",
      "------------- Epoch 17 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3399 - acc: 0.8865\n",
      "Dev-Accuracy: 75.68% (loss: 0.5667)\n",
      "Test-Accuracy: 74.89% (loss: 0.5883)\n",
      "\n",
      "------------- Epoch 18 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3563 - acc: 0.8767\n",
      "Dev-Accuracy: 74.62% (loss: 0.6013)\n",
      "Test-Accuracy: 73.43% (loss: 0.6245)\n",
      "\n",
      "------------- Epoch 19 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3324 - acc: 0.8912\n",
      "Dev-Accuracy: 75.23% (loss: 0.5944)\n",
      "Test-Accuracy: 74.21% (loss: 0.6176)\n",
      "\n",
      "------------- Epoch 20 ------------\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.3242 - acc: 0.8976\n",
      "Dev-Accuracy: 75.86% (loss: 0.5939)\n",
      "Test-Accuracy: 74.93% (loss: 0.6186)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit(X_train, y_train, batch_size=50, epochs=1, verbose=2)\n",
    "    \n",
    "    #Use Keras to compute the loss and the accuracy\n",
    "    dev_loss, dev_accuracy = model.evaluate(X_dev, y_dev, verbose=False)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    \n",
    "  \n",
    "    print(\"Dev-Accuracy: %.2f%% (loss: %.4f)\" % (dev_accuracy*100, dev_loss))\n",
    "    print(\"Test-Accuracy: %.2f%% (loss: %.4f)\" % (test_accuracy*100, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
