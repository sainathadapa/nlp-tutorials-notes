{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:56.320722Z",
     "start_time": "2017-11-06T06:54:56.246647Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:56.365705Z",
     "start_time": "2017-11-06T06:54:56.326987Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "    \n",
    "    print(filepath, len(sentences), 'sentences')\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:56.719496Z",
     "start_time": "2017-11-06T06:54:56.370101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sent-class-cnn/train.txt 5330 sentences\n",
      "data/sent-class-cnn/dev.txt 2664 sentences\n",
      "data/sent-class-cnn/test.txt 2668 sentences\n"
     ]
    }
   ],
   "source": [
    "traind = readFile('data/sent-class-cnn/train.txt')\n",
    "devd   = readFile('data/sent-class-cnn/dev.txt')\n",
    "testd  = readFile('data/sent-class-cnn/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:56.737540Z",
     "start_time": "2017-11-06T06:54:56.722032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'like',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'movies',\n",
       "  'with',\n",
       "  'more',\n",
       "  'elves',\n",
       "  'and',\n",
       "  'snow',\n",
       "  'and',\n",
       "  'less',\n",
       "  'pimps',\n",
       "  'and',\n",
       "  \"ho's\",\n",
       "  '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind[0][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:56.827742Z",
     "start_time": "2017-11-06T06:54:56.740237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind[1][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:57.072029Z",
     "start_time": "2017-11-06T06:54:56.832610Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "for dataset in [traind, devd, testd]:\n",
    "    for sentence in dataset[0]:\n",
    "        for word in sentence:\n",
    "            unique_words.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:57.216116Z",
     "start_time": "2017-11-06T06:54:57.074324Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words.add('__PADDING__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:54:57.366065Z",
     "start_time": "2017-11-06T06:54:57.220620Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:41.649365Z",
     "start_time": "2017-11-06T06:55:27.461250Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_index = {}\n",
    "f = open('data/glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:41.699730Z",
     "start_time": "2017-11-06T06:55:41.651939Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(unique_words), embedding_dim))\n",
    "for word, i in word_index_dict.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:41.757070Z",
     "start_time": "2017-11-06T06:55:41.702312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21348, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:58:06.598523Z",
     "start_time": "2017-10-15T16:58:06.593711Z"
    }
   },
   "source": [
    "## matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:44.796314Z",
     "start_time": "2017-11-06T06:55:44.775584Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrix(sentences):\n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    xMatrix = []\n",
    "    for sentence in sentences:\n",
    "        wordIndices = []\n",
    "        for word in sentence:\n",
    "            if word.lower() in word_index_dict:\n",
    "                wordIndices.append(word_index_dict[word.lower()])\n",
    "            else:\n",
    "                wordIndices.append(padding_index)\n",
    "        xMatrix.append(wordIndices)\n",
    "    \n",
    "    return xMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:45.305847Z",
     "start_time": "2017-11-06T06:55:45.153304Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mat = createMatrix(traind[0])\n",
    "dev_mat   = createMatrix(devd[0])\n",
    "test_mat  = createMatrix(testd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:45.631747Z",
     "start_time": "2017-11-06T06:55:45.624581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "28\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_mat[0]))\n",
    "print(len(train_mat[1]))\n",
    "print(len(train_mat[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:45.982490Z",
     "start_time": "2017-11-06T06:55:45.965223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_mat + dev_mat + test_mat:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "max_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:55:46.346377Z",
     "start_time": "2017-11-06T06:55:46.334925Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.array(traind[1])\n",
    "y_dev = np.array(devd[1])\n",
    "y_test = np.array(testd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:02.901638Z",
     "start_time": "2017-11-06T06:55:46.690847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:03.013973Z",
     "start_time": "2017-11-06T06:56:02.904013Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(train_mat, maxlen=max_sentence_len)\n",
    "X_dev = sequence.pad_sequences(dev_mat, maxlen=max_sentence_len)\n",
    "X_test = sequence.pad_sequences(test_mat, maxlen=max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:03.023849Z",
     "start_time": "2017-11-06T06:56:03.016671Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate, Embedding, Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:03.933067Z",
     "start_time": "2017-11-06T06:56:03.027604Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(max_sentence_len,), dtype='int32', name='words_input')\n",
    "wordsEmbeddingLayer = Embedding(embedding_matrix.shape[0],\n",
    "                                embedding_matrix.shape[1],                                     \n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "words = wordsEmbeddingLayer(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:04.047143Z",
     "start_time": "2017-11-06T06:56:03.936157Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we add a variable number of convolutions\n",
    "words_convolutions = []\n",
    "for filter_length in [1,2,3]:\n",
    "    words_conv = Convolution1D(filters=50,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "                            \n",
    "    words_conv = GlobalMaxPooling1D()(words_conv)      \n",
    "    \n",
    "    words_convolutions.append(words_conv)  \n",
    "\n",
    "output = concatenate(words_convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:04.097174Z",
     "start_time": "2017-11-06T06:56:04.049483Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:04.272301Z",
     "start_time": "2017-11-06T06:56:04.101545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, 59)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 59, 100)       2134800     words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 59, 50)        5050        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 59, 50)        10050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 59, 50)        15050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalMa (None, 50)            0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalMa (None, 50)            0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalMa (None, 50)            0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 150)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                   global_max_pooling1d_2[0][0]     \n",
      "                                                                   global_max_pooling1d_3[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 150)           0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           15100       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             101         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,180,151\n",
      "Trainable params: 45,351\n",
      "Non-trainable params: 2,134,800\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output = concatenate(words_convolutions)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(100, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "output = Dropout(0.25)(output)\n",
    "output = Dense(1, activation='sigmoid',  kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "model = Model(inputs=[words_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T06:56:37.554827Z",
     "start_time": "2017-11-06T06:56:04.274598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      "4s - loss: 1.6100 - acc: 0.5296\n",
      "Dev-Accuracy: 60.17% (loss: 1.2533)\n",
      "Test-Accuracy: 62.07% (loss: 1.2548)\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 1.1193 - acc: 0.6109\n",
      "Dev-Accuracy: 70.95% (loss: 0.9211)\n",
      "Test-Accuracy: 70.58% (loss: 0.9258)\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.8834 - acc: 0.6651\n",
      "Dev-Accuracy: 72.67% (loss: 0.7580)\n",
      "Test-Accuracy: 72.71% (loss: 0.7641)\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.7385 - acc: 0.7131\n",
      "Dev-Accuracy: 72.00% (loss: 0.6887)\n",
      "Test-Accuracy: 70.65% (loss: 0.6996)\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.6477 - acc: 0.7452\n",
      "Dev-Accuracy: 73.50% (loss: 0.6216)\n",
      "Test-Accuracy: 73.01% (loss: 0.6348)\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.5920 - acc: 0.7533\n",
      "Dev-Accuracy: 74.51% (loss: 0.5898)\n",
      "Test-Accuracy: 73.69% (loss: 0.6071)\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.5434 - acc: 0.7749\n",
      "Dev-Accuracy: 75.11% (loss: 0.5632)\n",
      "Test-Accuracy: 74.14% (loss: 0.5823)\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      "0s - loss: 0.5173 - acc: 0.7848\n",
      "Dev-Accuracy: 75.26% (loss: 0.5520)\n",
      "Test-Accuracy: 73.65% (loss: 0.5713)\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.4856 - acc: 0.8006\n",
      "Dev-Accuracy: 75.90% (loss: 0.5447)\n",
      "Test-Accuracy: 73.65% (loss: 0.5697)\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.4699 - acc: 0.8068\n",
      "Dev-Accuracy: 76.01% (loss: 0.5367)\n",
      "Test-Accuracy: 74.55% (loss: 0.5584)\n",
      "\n",
      "------------- Epoch 11 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.4498 - acc: 0.8169\n",
      "Dev-Accuracy: 76.39% (loss: 0.5349)\n",
      "Test-Accuracy: 74.21% (loss: 0.5602)\n",
      "\n",
      "------------- Epoch 12 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.4319 - acc: 0.8325\n",
      "Dev-Accuracy: 76.43% (loss: 0.5344)\n",
      "Test-Accuracy: 74.89% (loss: 0.5627)\n",
      "\n",
      "------------- Epoch 13 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.4094 - acc: 0.8458\n",
      "Dev-Accuracy: 76.31% (loss: 0.5464)\n",
      "Test-Accuracy: 74.63% (loss: 0.5759)\n",
      "\n",
      "------------- Epoch 14 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.3994 - acc: 0.8467\n",
      "Dev-Accuracy: 76.13% (loss: 0.5422)\n",
      "Test-Accuracy: 74.55% (loss: 0.5755)\n",
      "\n",
      "------------- Epoch 15 ------------\n",
      "Epoch 1/1\n",
      "0s - loss: 0.3851 - acc: 0.8610\n",
      "Dev-Accuracy: 76.50% (loss: 0.5524)\n",
      "Test-Accuracy: 74.85% (loss: 0.5880)\n",
      "\n",
      "------------- Epoch 16 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.3751 - acc: 0.8589\n",
      "Dev-Accuracy: 76.54% (loss: 0.5593)\n",
      "Test-Accuracy: 73.84% (loss: 0.5926)\n",
      "\n",
      "------------- Epoch 17 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.3650 - acc: 0.8732\n",
      "Dev-Accuracy: 77.18% (loss: 0.5528)\n",
      "Test-Accuracy: 74.89% (loss: 0.5853)\n",
      "\n",
      "------------- Epoch 18 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.3547 - acc: 0.8779\n",
      "Dev-Accuracy: 76.43% (loss: 0.5654)\n",
      "Test-Accuracy: 74.36% (loss: 0.5969)\n",
      "\n",
      "------------- Epoch 19 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.3372 - acc: 0.8887\n",
      "Dev-Accuracy: 77.29% (loss: 0.5632)\n",
      "Test-Accuracy: 74.78% (loss: 0.6058)\n",
      "\n",
      "------------- Epoch 20 ------------\n",
      "Epoch 1/1\n",
      "1s - loss: 0.3290 - acc: 0.8932\n",
      "Dev-Accuracy: 76.50% (loss: 0.5754)\n",
      "Test-Accuracy: 74.33% (loss: 0.6215)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit(X_train, y_train, batch_size=50, epochs=1, verbose=2)\n",
    "    \n",
    "    #Use Keras to compute the loss and the accuracy\n",
    "    dev_loss, dev_accuracy = model.evaluate(X_dev, y_dev, verbose=False)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    \n",
    "  \n",
    "    print(\"Dev-Accuracy: %.2f%% (loss: %.4f)\" % (dev_accuracy*100, dev_loss))\n",
    "    print(\"Test-Accuracy: %.2f%% (loss: %.4f)\" % (test_accuracy*100, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
