{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:48:42.171952Z",
     "start_time": "2017-10-15T16:48:42.090572Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:53:23.778676Z",
     "start_time": "2017-10-15T16:53:23.757102Z"
    }
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath):\n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "    \n",
    "    print(filepath, len(sentences), 'sentences')\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:53:24.143815Z",
     "start_time": "2017-10-15T16:53:24.075442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sent-class-cnn/train.txt 5330 sentences\n",
      "./sent-class-cnn/dev.txt 2664 sentences\n",
      "./sent-class-cnn/test.txt 2668 sentences\n"
     ]
    }
   ],
   "source": [
    "traind = readFile('./sent-class-cnn/train.txt')\n",
    "devd   = readFile('./sent-class-cnn/dev.txt')\n",
    "testd  = readFile('./sent-class-cnn/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:53:50.669151Z",
     "start_time": "2017-10-15T16:53:50.660598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'like',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'movies',\n",
       "  'with',\n",
       "  'more',\n",
       "  'elves',\n",
       "  'and',\n",
       "  'snow',\n",
       "  'and',\n",
       "  'less',\n",
       "  'pimps',\n",
       "  'and',\n",
       "  \"ho's\",\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind[0][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:54:01.392576Z",
     "start_time": "2017-10-15T16:54:01.385916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind[1][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:55:46.165555Z",
     "start_time": "2017-10-15T16:55:46.070014Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "for dataset in [traind, devd, testd]:\n",
    "    for sentence in dataset[0]:\n",
    "        for word in sentence:\n",
    "            unique_words.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:55:58.210056Z",
     "start_time": "2017-10-15T16:55:58.204154Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words.add('__PADDING__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:56:04.161975Z",
     "start_time": "2017-10-15T16:56:04.140514Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_dict = dict([(x,i) for i,x in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:56:39.196568Z",
     "start_time": "2017-10-15T16:56:23.948572Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_index = {}\n",
    "f = open('./glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:56:39.248564Z",
     "start_time": "2017-10-15T16:56:39.198720Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(unique_words), embedding_dim))\n",
    "for word, i in word_index_dict.items():\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:56:44.626583Z",
     "start_time": "2017-10-15T16:56:44.620511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21348, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T16:58:06.598523Z",
     "start_time": "2017-10-15T16:58:06.593711Z"
    }
   },
   "source": [
    "## matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:02:13.262907Z",
     "start_time": "2017-10-15T17:02:13.243713Z"
    }
   },
   "outputs": [],
   "source": [
    "def createMatrix(sentences):\n",
    "    padding_index = word_index_dict['__PADDING__']\n",
    "    \n",
    "    xMatrix = []\n",
    "    for sentence in sentences:\n",
    "        wordIndices = []\n",
    "        for word in sentence:\n",
    "            if word.lower() in word_index_dict:\n",
    "                wordIndices.append(word_index_dict[word.lower()])\n",
    "            else:\n",
    "                wordIndices.append(padding_index)\n",
    "        xMatrix.append(wordIndices)\n",
    "    \n",
    "    return xMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:02:54.291094Z",
     "start_time": "2017-10-15T17:02:54.122983Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mat = createMatrix(traind[0])\n",
    "dev_mat   = createMatrix(devd[0])\n",
    "test_mat  = createMatrix(testd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:03:41.572560Z",
     "start_time": "2017-10-15T17:03:41.563894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "28\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_mat[0]))\n",
    "print(len(train_mat[1]))\n",
    "print(len(train_mat[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:08:15.508631Z",
     "start_time": "2017-10-15T17:08:15.490034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_mat + dev_mat + test_mat:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "max_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:08:47.825896Z",
     "start_time": "2017-10-15T17:08:47.819298Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.array(traind[1])\n",
    "y_dev = np.array(devd[1])\n",
    "y_test = np.array(testd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:10:01.583212Z",
     "start_time": "2017-10-15T17:09:45.032591Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:10:02.858173Z",
     "start_time": "2017-10-15T17:10:02.713640Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(train_mat, maxlen=max_sentence_len)\n",
    "X_dev = sequence.pad_sequences(dev_mat, maxlen=max_sentence_len)\n",
    "X_test = sequence.pad_sequences(test_mat, maxlen=max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:13:37.130754Z",
     "start_time": "2017-10-15T17:13:37.122000Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate, Embedding, Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:13:38.711125Z",
     "start_time": "2017-10-15T17:13:38.640847Z"
    }
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(max_sentence_len,), dtype='int32', name='words_input')\n",
    "wordsEmbeddingLayer = Embedding(embedding_matrix.shape[0],\n",
    "                                embedding_matrix.shape[1],                                     \n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "words = wordsEmbeddingLayer(words_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:14:16.069664Z",
     "start_time": "2017-10-15T17:14:15.969914Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we add a variable number of convolutions\n",
    "words_convolutions = []\n",
    "for filter_length in [1,2,3]:\n",
    "    words_conv = Convolution1D(filters=50,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "                            \n",
    "    words_conv = GlobalMaxPooling1D()(words_conv)      \n",
    "    \n",
    "    words_convolutions.append(words_conv)  \n",
    "\n",
    "output = concatenate(words_convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:15:17.624195Z",
     "start_time": "2017-10-15T17:15:17.620169Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:15:18.304761Z",
     "start_time": "2017-10-15T17:15:18.187448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, 59)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 59, 100)       2134800     words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 59, 50)        5050        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 59, 50)        10050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 59, 50)        15050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalMa (None, 50)            0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalMa (None, 50)            0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalMa (None, 50)            0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 150)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                   global_max_pooling1d_2[0][0]     \n",
      "                                                                   global_max_pooling1d_3[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 150)           0           concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           15100       dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             101         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,180,151\n",
      "Trainable params: 45,351\n",
      "Non-trainable params: 2,134,800\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output = concatenate(words_convolutions)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(100, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "output = Dropout(0.25)(output)\n",
    "output = Dense(1, activation='sigmoid',  kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "model = Model(inputs=[words_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T17:17:18.234948Z",
     "start_time": "2017-10-15T17:16:05.031281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Epoch 1 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 1.1235 - acc: 0.6163\n",
      "Dev-Accuracy: 70.42% (loss: 0.9353)\n",
      "Test-Accuracy: 69.19% (loss: 0.9403)\n",
      "\n",
      "------------- Epoch 2 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.8673 - acc: 0.6856\n",
      "Dev-Accuracy: 74.06% (loss: 0.7645)\n",
      "Test-Accuracy: 72.34% (loss: 0.7714)\n",
      "\n",
      "------------- Epoch 3 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.7225 - acc: 0.7308\n",
      "Dev-Accuracy: 72.97% (loss: 0.6841)\n",
      "Test-Accuracy: 72.75% (loss: 0.6916)\n",
      "\n",
      "------------- Epoch 4 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.6388 - acc: 0.7463\n",
      "Dev-Accuracy: 73.54% (loss: 0.6231)\n",
      "Test-Accuracy: 73.24% (loss: 0.6308)\n",
      "\n",
      "------------- Epoch 5 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.5745 - acc: 0.7679\n",
      "Dev-Accuracy: 74.96% (loss: 0.5817)\n",
      "Test-Accuracy: 74.66% (loss: 0.5915)\n",
      "\n",
      "------------- Epoch 6 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.5321 - acc: 0.7824\n",
      "Dev-Accuracy: 75.26% (loss: 0.5618)\n",
      "Test-Accuracy: 74.59% (loss: 0.5729)\n",
      "\n",
      "------------- Epoch 7 ------------\n",
      "Epoch 1/1\n",
      "3s - loss: 0.5081 - acc: 0.7914\n",
      "Dev-Accuracy: 75.49% (loss: 0.5512)\n",
      "Test-Accuracy: 75.26% (loss: 0.5609)\n",
      "\n",
      "------------- Epoch 8 ------------\n",
      "Epoch 1/1\n",
      "3s - loss: 0.4787 - acc: 0.8039\n",
      "Dev-Accuracy: 75.90% (loss: 0.5474)\n",
      "Test-Accuracy: 74.70% (loss: 0.5593)\n",
      "\n",
      "------------- Epoch 9 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.4577 - acc: 0.8122\n",
      "Dev-Accuracy: 71.96% (loss: 0.6024)\n",
      "Test-Accuracy: 69.83% (loss: 0.6128)\n",
      "\n",
      "------------- Epoch 10 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.4435 - acc: 0.8244\n",
      "Dev-Accuracy: 75.86% (loss: 0.5424)\n",
      "Test-Accuracy: 74.40% (loss: 0.5530)\n",
      "\n",
      "------------- Epoch 11 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.4249 - acc: 0.8368\n",
      "Dev-Accuracy: 76.24% (loss: 0.5452)\n",
      "Test-Accuracy: 75.04% (loss: 0.5584)\n",
      "\n",
      "------------- Epoch 12 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.4165 - acc: 0.8379\n",
      "Dev-Accuracy: 76.01% (loss: 0.5469)\n",
      "Test-Accuracy: 74.89% (loss: 0.5665)\n",
      "\n",
      "------------- Epoch 13 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3954 - acc: 0.8497\n",
      "Dev-Accuracy: 75.19% (loss: 0.5699)\n",
      "Test-Accuracy: 74.29% (loss: 0.5832)\n",
      "\n",
      "------------- Epoch 14 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3844 - acc: 0.8557\n",
      "Dev-Accuracy: 76.35% (loss: 0.5527)\n",
      "Test-Accuracy: 74.96% (loss: 0.5766)\n",
      "\n",
      "------------- Epoch 15 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3699 - acc: 0.8660\n",
      "Dev-Accuracy: 76.80% (loss: 0.5535)\n",
      "Test-Accuracy: 74.96% (loss: 0.5803)\n",
      "\n",
      "------------- Epoch 16 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3614 - acc: 0.8694\n",
      "Dev-Accuracy: 75.98% (loss: 0.5689)\n",
      "Test-Accuracy: 74.40% (loss: 0.5916)\n",
      "\n",
      "------------- Epoch 17 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3499 - acc: 0.8779\n",
      "Dev-Accuracy: 75.90% (loss: 0.5740)\n",
      "Test-Accuracy: 75.37% (loss: 0.6029)\n",
      "\n",
      "------------- Epoch 18 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3318 - acc: 0.8921\n",
      "Dev-Accuracy: 76.88% (loss: 0.5770)\n",
      "Test-Accuracy: 75.04% (loss: 0.6030)\n",
      "\n",
      "------------- Epoch 19 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3322 - acc: 0.8884\n",
      "Dev-Accuracy: 76.05% (loss: 0.5907)\n",
      "Test-Accuracy: 74.10% (loss: 0.6184)\n",
      "\n",
      "------------- Epoch 20 ------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3252 - acc: 0.8938\n",
      "Dev-Accuracy: 76.39% (loss: 0.5840)\n",
      "Test-Accuracy: 74.40% (loss: 0.6124)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "    model.fit(X_train, y_train, batch_size=50, epochs=1, verbose=2)\n",
    "    \n",
    "    #Use Keras to compute the loss and the accuracy\n",
    "    dev_loss, dev_accuracy = model.evaluate(X_dev, y_dev, verbose=False)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    \n",
    "  \n",
    "    print(\"Dev-Accuracy: %.2f%% (loss: %.4f)\" % (dev_accuracy*100, dev_loss))\n",
    "    print(\"Test-Accuracy: %.2f%% (loss: %.4f)\" % (test_accuracy*100, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
